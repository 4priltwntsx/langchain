{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {},
   "source": [
    "# Adaptive RAG\n",
    "\n",
    "이 튜토리얼은 Adaptive RAG(Adaptive Retrieval-Augmented Generation)의 구현을 다룹니다. \n",
    "\n",
    "Adaptive RAG는 쿼리 분석과 능동적/자기 수정 RAG를 결합하여 다양한 데이터 소스에서 정보를 검색하고 생성하는 전략입니다. \n",
    "\n",
    "이 튜토리얼에서는 LangGraph를 사용하여 웹 검색과 자기 수정 RAG 간의 라우팅을 구현합니다.\n",
    "\n",
    "**주로 다루는 내용**\n",
    "\n",
    "- **Create Index**: 인덱스 생성 및 문서 로드\n",
    "- **LLMs**: LLM을 사용한 쿼리 라우팅 및 문서 평가\n",
    "- **Web Search Tool**: 웹 검색 도구 설정\n",
    "- **Construct the Graph**: 그래프 상태 및 흐름 정의\n",
    "- **Compile Graph**: 그래프 컴파일 및 워크플로우 구축\n",
    "- **Use Graph**: 그래프 실행 및 결과 확인\n",
    "\n",
    "----\n",
    "\n",
    "**Adaptive RAG**는 **RAG**의 전략으로, (1) [쿼리 분석](https://blog.langchain.dev/query-construction/)과 (2) [Self-Reflective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/)을 결합합니다.\n",
    "\n",
    "[논문: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity](https://arxiv.org/abs/2403.14403) 에서는 쿼리 분석을 통해 다음과 같은 라우팅을 수행합니다.\n",
    "\n",
    "- `No Retrieval`\n",
    "- `Single-shot RAG`\n",
    "- `Iterative RAG`\n",
    "\n",
    "LangGraph를 사용하여 이를 구현합니다.\n",
    "\n",
    "이 구현에서는 다음과 같은 라우팅을 수행합니다.\n",
    "\n",
    "- **웹 검색**: 최신 이벤트와 관련된 질문에 사용\n",
    "- **자기 수정 RAG**: 인덱스와 관련된 질문에 사용\n",
    "\n",
    "![adaptive-rag.png](./assets/langgraph-adaptive-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb73bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25ec196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9065ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH13-LangGraph-Structures\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH13-LangGraph-Structures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00c3f4",
   "metadata": {},
   "source": [
    "## 기본 PDF 기반 Retrieval Chain 생성\n",
    "\n",
    "여기서는 PDF 문서를 기반으로 Retrieval Chain 을 생성합니다. 가장 단순한 구조의 Retrieval Chain 입니다.\n",
    "\n",
    "단, LangGraph 에서는 Retirever 와 Chain 을 따로 생성합니다. 그래야 각 노드별로 세부 처리를 할 수 있습니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 이전 튜토리얼에서 다룬 내용이므로, 자세한 설명은 생략합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69cb77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 로드합니다.\n",
    "pdf = PDFRetrievalChain([\"data/SPRI_AI_Brief_2023년12월호_F.pdf\"]).create_chain()\n",
    "\n",
    "# retriever 생성\n",
    "pdf_retriever = pdf.retriever\n",
    "\n",
    "# chain 생성\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fc536",
   "metadata": {},
   "source": [
    "## 쿼리 라우팅과 문서 평가\n",
    "\n",
    "**LLMs** 단계에서는 **쿼리 라우팅**과 **문서 평가**를 수행합니다. 이 과정은 **Adaptive RAG**의 중요한 부분으로, 효율적인 정보 검색과 생성에 기여합니다.\n",
    "\n",
    "- **쿼리 라우팅**: 사용자의 쿼리를 분석하여 적절한 정보 소스로 라우팅합니다. 이를 통해 쿼리의 목적에 맞는 최적의 검색 경로를 설정할 수 있습니다.\n",
    "- **문서 평가**: 검색된 문서의 품질과 관련성을 평가하여 최종 결과의 정확성을 높입니다. 이 과정은 **LLMs**의 성능을 극대화하는 데 필수적입니다.\n",
    "\n",
    "이 단계는 **Adaptive RAG**의 핵심 기능을 지원하며, 정확하고 신뢰할 수 있는 정보 제공을 목표로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b78d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 최신 LLM 모델 이름 가져오기\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4)\n",
    "\n",
    "\n",
    "# 사용자 쿼리를 가장 관련성 높은 데이터 소스로 라우팅하는 데이터 모델\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    # 데이터 소스 선택을 위한 리터럴 타입 필드\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 초기화 및 함수 호출을 통한 구조화된 출력 생성\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# 시스템 메시지와 사용자 질문을 포함한 프롬프트 템플릿 생성\n",
    "## LLM이 시스템 프롬프트 내용을 보고 어떤 도구를 사용할지 결정 -> 자세하게 작성\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to DEC 2023 AI Brief Report(SPRI) with Samsung Gause, Anthropic, etc.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "\n",
    "# Routing 을 위한 프롬프트 템플릿 생성\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿과 구조화된 LLM 라우터를 결합하여 질문 라우터 생성\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e41f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='vectorstore')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RouteQuery(datasource=\"vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4d831",
   "metadata": {},
   "source": [
    "다음은 쿼리 라우팅 결과를 테스트 해본 뒤 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0874c14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "# 문서 검색이 필요한 질문\n",
    "print(\n",
    "    question_router.invoke(\n",
    "        {\"question\": \"AI Brief 에서 삼성전자가 만든 생성형 AI 의 이름은?\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d22b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "# 웹 검색이 필요한 질문\n",
    "print(question_router.invoke({\"question\": \"판교에서 가장 맛있는 딤섬집 찾아줘\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc43b99",
   "metadata": {},
   "source": [
    "### 검색 평가기(Retrieval Grader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1221d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# 문서 평가를 위한 데이터 모델 정의\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 초기화 및 함수 호출을 통한 구조화된 출력 생성\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# 시스템 메시지와 사용자 질문을 포함한 프롬프트 템플릿 생성\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 문서 검색결과 평가기 생성\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927cac10",
   "metadata": {},
   "source": [
    "생성한 `retrieval_grader` 를 사용하여 문서 검색결과를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa5e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 질문 설정\n",
    "question = \"삼성전자가 만든 생성형 AI 의 이름은?\"\n",
    "\n",
    "# 질문에 대한 관련 문서 검색\n",
    "docs = pdf_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef397b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# 검색된 문서의 내용 가져오기\n",
    "retrieved_doc = docs[1].page_content\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": retrieved_doc}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce41bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링 하는 코드 예시\n",
    "filtered_docs = []\n",
    "for doc in docs:\n",
    "    result = retrieval_grader.invoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"document\": doc.page_content,\n",
    "        }\n",
    "    )\n",
    "    if result.binary_score == \"yes\":\n",
    "        filtered_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dce7a1",
   "metadata": {},
   "source": [
    "### 답변 생성을 위한 RAG 체인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "992ef15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChain Hub에서 프롬프트 가져오기(RAG 프롬프트는 자유롭게 수정 가능)\n",
    "prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
    "\n",
    "\n",
    "# 문서 포맷팅 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# RAG 체인 생성\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc96e3",
   "metadata": {},
   "source": [
    "이제 생성한 `rag_chain` 에 질문을 전달하여 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8d16e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 만든 생성형 AI의 이름은 '삼성 가우스'입니다.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023년12월호_F.pdf (page 13)\n"
     ]
    }
   ],
   "source": [
    "# RAG 체인에 질문을 전달하여 답변 생성\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9f601",
   "metadata": {},
   "source": [
    "### 답변의 Hallucination 체커 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40ec0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 할루시네이션 체크를 위한 데이터 모델 정의\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 함수 호출을 통한 LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# 프롬프트 설정\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 환각 평가기 생성\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550b7cf",
   "metadata": {},
   "source": [
    "생성한 `hallucination_grader` 를 사용하여 생성된 답변의 환각 여부를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb593684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='no')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가기를 사용하여 생성된 답변의 환각 여부 평가\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "110eb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary scoring to evaluate the appropriateness of answers to questions\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicate 'yes' or 'no' whether the answer solves the question\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 함수 호출을 통한 LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# 프롬프트 설정\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿과 구조화된 LLM 평가기를 결합하여 답변 평가기 생성\n",
    "answer_grader = answer_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a26ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가기를 사용하여 생성된 답변이 질문을 해결하는지 여부 평가\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc11dd",
   "metadata": {},
   "source": [
    "### 쿼리 재작성(Query Rewriter)\n",
    "\n",
    "- 검증을 하기 위해서. \n",
    "- 질의어하고 관련이 있는지 없는지 판단을 해서.\n",
    "- 답변이 연관성이 있는지 없는지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9df325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# Query Rewriter 프롬프트 정의(자유롭게 수정이 가능합니다)\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "# Query Rewriter 프롬프트 템플릿 생성\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Query Rewriter 생성\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd3e83",
   "metadata": {},
   "source": [
    "생성한 `question_rewriter` 에 질문을 전달하여 개선된 질문을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6eb92e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성전자가 개발한 생성형 AI의 명칭은 무엇인가요?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문 재작성기에 질문을 전달하여 개선된 질문 생성\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5ee42",
   "metadata": {},
   "source": [
    "### 웹 검색 도구\n",
    "\n",
    "**웹 검색 도구**는 **Adaptive RAG**의 중요한 구성 요소로, 최신 정보를 검색하는 데 사용됩니다. 이 도구는 사용자가 최신 이벤트와 관련된 질문에 대해 신속하고 정확한 답변을 얻을 수 있도록 지원합니다.\n",
    "\n",
    "- **설정**: 웹 검색 도구를 설정하여 최신 정보를 검색할 수 있도록 준비합니다.\n",
    "- **검색 수행**: 사용자의 쿼리를 기반으로 웹에서 관련 정보를 검색합니다.\n",
    "- **결과 분석**: 검색된 결과를 분석하여 사용자의 질문에 가장 적합한 정보를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e004263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# 웹 검색 도구 생성\n",
    "web_search_tool = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d60abe",
   "metadata": {},
   "source": [
    "웹 검색 도구를 실행하여 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c13be8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) - 테디노트', 'url': 'https://teddylee777.github.io/langchain/langchain-tutorial-01/', 'content': '② LangChain 한국어 튜토리얼 바로가기 👀 ③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌 ④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌 ⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌 랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) 2023년 09월 28일 5 분 소요 목차 🌱 랭체인의 주요 기능 🌱 환경설정 API KEY 발급 모듈 설치(openai, langchain) 🔥 ChatOpenAI 🔥 프롬프트 템플릿의 활용 LLMChain 객체 ① run() ② apply() ③ generate() ④ 2개 이상의 변수를 템플릿 안에 정의 ⑤ 스트리밍(streaming) 언어 모델을 활용한 애플리케이션 개발을 돕는 프레임워크인 랭체인(LangChain) 에 대해 깊이 있게 다뤄보고자 합니다. 튜토리얼은 시리즈 형식으로 구성되어, 시리즈를 거듭하면서 랭체인(LangChain) 을 통해 언어 모델 기반의 애플리케이션 개발은 더욱 간결하고 효과적으로 이루어질 수 있습니다. 추론 능력: 제공된 문맥에 기반하여 어떤 대답을 할지, 또는 어떠한 액션을 취할지에 대한 추론이 가능합니다. 특히, 이러한 사용 준비된 체인은 초보자도 랭체인을 쉽게 시작할 수 있게 도와주며, 복잡한 애플리케이션을 계획하는 전문가들은 기존 체인을 손쉽게 커스터마이징하거나 새롭게 구축할 수 있게 도와줍니다.', 'score': 0.6506676, 'raw_content': \"랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) - 테디노트\\n\\nSkip to primary navigation\\nSkip to content\\nSkip to footer\\n\\n테디노트 데이터와 인공지능을 좋아하는 개발자 노트\\n\\n검색\\n카테고리\\n태그\\n연도\\n강의\\n어바웃미\\n\\n토글 메뉴\\n\\nHome \\n/3.  Langchain \\n/5.  랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\\n\\n🔥알림🔥\\n① 테디노트 유튜브 - 구경하러 가기!\\n② LangChain 한국어 튜토리얼 바로가기 👀\\n③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌\\n④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌\\n⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌\\n랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\\n2023년 09월 28일 5 분 소요\\n목차\\n\\n🌱 랭체인의 주요 기능\\n🌱 환경설정\\nAPI KEY 발급\\n모듈 설치(openai, langchain)\\n\\n\\n🔥 ChatOpenAI\\n🔥 프롬프트 템플릿의 활용\\nLLMChain 객체\\n① run()\\n② apply()\\n③ generate()\\n④ 2개 이상의 변수를 템플릿 안에 정의\\n⑤ 스트리밍(streaming)\\n\\n\\n\\n언어 모델을 활용한 애플리케이션 개발을 돕는 프레임워크인 랭체인(LangChain) 에 대해 깊이 있게 다뤄보고자 합니다.\\n튜토리얼은 시리즈 형식으로 구성되어, 시리즈를 거듭하면서 랭체인(LangChain) 을 통해 언어 모델 기반의 애플리케이션 개발은 더욱 간결하고 효과적으로 이루어질 수 있습니다.\\n🌱 랭체인의 주요 기능\\n랭체인을 통해 다음과 같은 특징을 갖는 애플리케이션을 개발할 수 있습니다.\\n\\n문맥 인식: 언어 모델과 다양한 문맥 소스(프롬프트 지시, 예제, 응답의 근거 내용 등)를 연동하며, 사용자의 문맥을 정확히 이해합니다.\\n추론 능력: 제공된 문맥에 기반하여 어떤 대답을 할지, 또는 어떠한 액션을 취할지에 대한 추론이 가능합니다.\\n\\n랭체인의 가치\\n랭체인의 핵심적인 가치는 여러 가지가 있지만, 그 중에서도 두 가지 주요한 점을 꼽자면 다음과 같습니다.\\n\\n구성 요소: 사용자는 언어 모델과의 상호작용을 위해 다양한 구성 요소와 추상화를 활용할 수 있습니다. 이러한 구성 요소는 개별적으로, 또는 랭체인 프레임워크 내에서 모듈식으로 쉽게 활용할 수 있습니다.\\n사용 준비된 체인: 특정 고수준 작업을 수행하기 위해 미리 조립된 구성 요소의 패키지입니다.\\n\\n특히, 이러한 사용 준비된 체인은 초보자도 랭체인을 쉽게 시작할 수 있게 도와주며, 복잡한 애플리케이션을 계획하는 전문가들은 기존 체인을 손쉽게 커스터마이징하거나 새롭게 구축할 수 있게 도와줍니다.\\n🌱 환경설정\\nAPI KEY 발급\\n먼저, openai 의 API KEY 를 발급 받아야 합니다. 발급은 다음의 절차를 통해 진행할 수 있습니다.\\nhttps://platform.openai.com/account/api-keys 로 접속합니다.\\n\\nLog in 버튼을 클릭 후 계정에 로그인 합니다. 계정이 아직 생성되지 않은 경우에는 Sign up 으로 회원가입 후 로그인 합니다.\\n\\n\\n\\n“Create new secret key” 버튼을 클릭하여 새로운 키를 발급합니다.\\n\\n\\n\\nName 에는 발급하는 키에 대한 별칭을 입력합니다.\\n\\n\\n\\n새롭게 발급한 키를 복사합니다. 잃어버리면 다시 발급하여야 하므로, 안전한 곳에 저장해 둡니다.\\n\\n\\n모듈 설치(openai, langchain)\\npip 명령어로 모듈을 설치 합니다. 아나콘다 가상환경에서 설치해도 좋습니다.\\n```\\nopenai 파이썬 패키지 설치\\npip install openai langchain\\n```\\n먼저, 설치한 openai 모듈을 import 한 뒤, 발급받은 API KEY를 다음과 같이 설정합니다.\\n```\\nimport os\\nos.environ['OPENAI_API_KEY'] = 'OPENAI API KEY 입력'\\n```\\n사용 가능한 모델 리스트 출력\\n```\\nimport openai\\nmodel_list = sorted([m['id'] for m in openai.Model.list()['data']])\\nfor m in model_list:\\n    print(m)\\n```\\nada\\nada-code-search-code\\nada-code-search-text\\nada-search-document\\nada-search-query\\nada-similarity\\nbabbage\\nbabbage-002\\nbabbage-code-search-code\\nbabbage-code-search-text\\nbabbage-search-document\\nbabbage-search-query\\nbabbage-similarity\\ncode-davinci-edit-001\\ncode-search-ada-code-001\\ncode-search-ada-text-001\\ncode-search-babbage-code-001\\ncode-search-babbage-text-001\\ncurie\\ncurie-instruct-beta\\ncurie-search-document\\ncurie-search-query\\ncurie-similarity\\ndavinci\\ndavinci-002\\ndavinci-instruct-beta\\ndavinci-search-document\\ndavinci-search-query\\ndavinci-similarity\\ngpt-3.5-turbo\\ngpt-3.5-turbo-0301\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k\\ngpt-3.5-turbo-16k-0613\\ngpt-3.5-turbo-instruct\\ngpt-3.5-turbo-instruct-0914\\ngpt-4\\ngpt-4-0314\\ngpt-4-0613\\ntext-ada-001\\ntext-babbage-001\\ntext-curie-001\\ntext-davinci-001\\ntext-davinci-002\\ntext-davinci-003\\ntext-davinci-edit-001\\ntext-embedding-ada-002\\ntext-search-ada-doc-001\\ntext-search-ada-query-001\\ntext-search-babbage-doc-001\\ntext-search-babbage-query-001\\ntext-search-curie-doc-001\\ntext-search-curie-query-001\\ntext-search-davinci-doc-001\\ntext-search-davinci-query-001\\ntext-similarity-ada-001\\ntext-similarity-babbage-001\\ntext-similarity-curie-001\\ntext-similarity-davinci-001\\nwhisper-1\\n🔥 ChatOpenAI\\nOpenAI 사의 채팅 전용 Large Language Model(llm) 입니다.\\n객체를 생성할 때 다음을 옵션 값을 지정할 수 있습니다. 옵션에 대한 상세 설명은 다음과 같습니다.\\ntemperature\\n\\n사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.\\n\\nmax_tokens\\n\\n채팅 완성에서 생성할 토큰의 최대 개수입니다.\\n\\nmodel_name: 적용 가능한 모델 리스트\\n\\n\\ngpt-3.5-turbo\\n\\n\\ngpt-3.5-turbo-0301\\n\\n\\ngpt-3.5-turbo-0613\\n\\n\\ngpt-3.5-turbo-16k\\n\\n\\ngpt-3.5-turbo-16k-0613\\n\\n\\ngpt-3.5-turbo-instruct\\n\\n\\ngpt-3.5-turbo-instruct-0914\\n\\n\\ngpt-4\\n\\n\\ngpt-4-0314\\n\\n\\ngpt-4-0613\\n\\n\\n```\\nfrom langchain.chat_models import ChatOpenAI\\n객체 생성\\nllm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \\n                 max_tokens=2048,             # 최대 토큰수\\n                 model_name='gpt-3.5-turbo',  # 모델명\\n                )\\n질의내용\\nquestion = '대한민국의 수도는 뭐야?'\\n질의\\nprint(f'[답변]: {llm.predict(question)}')\\n```\\n[답변]: 대한민국의 수도는 서울입니다.\\n🔥 프롬프트 템플릿의 활용\\nPromptTemplate\\n\\n\\n사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\\n\\n\\n사용법\\n\\n\\ntemplate: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 {}는 변수를 나타냅니다.\\n\\n\\ninput_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\\n\\n\\n\\n\\ninput_variables\\n\\n\\ninput_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\\n\\n\\n사용법: 리스트 형식으로 변수 이름을 정의합니다.\\n\\n\\n```\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n질문 템플릿 형식 정의\\ntemplate = '{country}의 수도는 뭐야?'\\n템플릿 완성\\nprompt = PromptTemplate(template=template, input_variables=['country'])\\n```\\nLLMChain 객체\\nLLMChain\\n\\n\\nLLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다\\n\\n\\n사용법\\n\\n\\nprompt: 앞서 정의한 PromptTemplate 객체를 사용합니다.\\n\\n\\nllm: 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다.\\n\\n\\n\\n\\n```\\n연결된 체인(Chain)객체 생성\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n```\\n① run()\\nrun() 함수로 템플릿 프롬프트 실행\\n```\\n체인 실행: run()\\nprint(llm_chain.run(country='일본'))\\n```\\n일본의 수도는 도쿄입니다.\\n```\\n체인 실행: run()\\nprint(llm_chain.run(country='캐나다'))\\n```\\n캐나다의 수도는 오타와(Ottawa)입니다.\\n② apply()\\napply() 함수로 여러개의 입력을 한 번에 실행\\n```\\ninput_list = [\\n    {'country': '호주'},\\n    {'country': '중국'},\\n    {'country': '네덜란드'}\\n]\\nllm_chain.apply(input_list)\\n```\\n[{'text': '호주의 수도는 캔버라입니다.'},\\n {'text': '중국의 수도는 베이징(北京)입니다.'},\\n {'text': '네덜란드의 수도는 암스테르담(Amsterdam)입니다.'}]\\ntext 키 값으로 결과 뭉치가 반환되었음을 확인할 수 있습니다.\\n이를 반복문으로 출력한다면 다음과 같습니다.\\n```\\ninput_list 에 대한 결과 반환\\nresult = llm_chain.apply(input_list)\\n반복문으로 결과 출력\\nfor res in result:\\n    print(res['text'].strip())\\n```\\n호주의 수도는 캔버라입니다.\\n중국의 수도는 베이징(北京)입니다.\\n네덜란드의 수도는 암스테르담(Amsterdam)입니다.\\n③ generate()\\ngenerate() 는 문자열 대신에 LLMResult를 반환하는 점을 제외하고는 apply와 유사합니다.\\nLLMResult는 토큰 사용량과 종료 이유와 같은 유용한 생성 정보를 자주 포함하고 있습니다.\\n```\\ninput_list 에 대한 결과 반환\\ngenerated_result = llm_chain.generate(input_list)\\nprint(generated_result)\\n```\\ngenerations=[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='호주의 수도는 캔버라입니다.', additional_kwargs={}, example=False))], [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.', additional_kwargs={}, example=False))], [ChatGeneration(text='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 58, 'completion_tokens': 57, 'total_tokens': 115}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('957a5369-a20e-470a-bcea-c325b3aafb4a')), RunInfo(run_id=UUID('f5f6f639-76f8-43e3-9103-03aa7eac6fe5')), RunInfo(run_id=UUID('f9c4ce3f-4e5d-47d5-86af-f20c077b754e'))]\\n```\\n답변 출력\\ngenerated_result.generations\\n```\\n[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='호주의 수도는 캔버라입니다.', additional_kwargs={}, example=False))],\\n [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.', additional_kwargs={}, example=False))],\\n [ChatGeneration(text='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', additional_kwargs={}, example=False))]]\\n```\\n토큰 사용량 출력\\ngenerated_result.llm_output\\n```\\n{'token_usage': {'prompt_tokens': 58,\\n  'completion_tokens': 57,\\n  'total_tokens': 115},\\n 'model_name': 'gpt-3.5-turbo'}\\n```\\nrun ID 출력\\ngenerated_result.run\\n```\\n[RunInfo(run_id=UUID('957a5369-a20e-470a-bcea-c325b3aafb4a')),\\n RunInfo(run_id=UUID('f5f6f639-76f8-43e3-9103-03aa7eac6fe5')),\\n RunInfo(run_id=UUID('f9c4ce3f-4e5d-47d5-86af-f20c077b754e'))]\\n```\\n답변 출력\\nfor gen in generated_result.generations:\\n    print(gen[0].text.strip())\\n```\\n호주의 수도는 캔버라입니다.\\n중국의 수도는 베이징(北京)입니다.\\n네덜란드의 수도는 암스테르담(Amsterdam)입니다.\\n④ 2개 이상의 변수를 템플릿 안에 정의\\n2개 이상의 변수를 적용하여 템플릿을 생성할 수 있습니다.\\n이번에는 2개 이상의 변수(input_variables) 를 활용하여 템플릿 구성을 해보겠습니다.\\n```\\n질문 템플릿 형식 정의\\ntemplate = '{area1} 와 {area2} 의 시차는 몇시간이야?'\\n템플릿 완성\\nprompt = PromptTemplate(template=template, input_variables=['area1', 'area2'])\\n연결된 체인(Chain)객체 생성\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n```\\n```\\n체인 실행: run()\\nprint(llm_chain.run(area1='서울', area2='파리'))\\n```\\n서울과 파리의 시차는 8시간입니다. 서울이 파리보다 8시간 앞서 있습니다.\\n```\\ninput_list = [\\n    {'area1': '파리', 'area2': '뉴욕'},\\n    {'area1': '서울', 'area2': '하와이'},\\n    {'area1': '켄버라', 'area2': '베이징'}\\n]\\n반복문으로 결과 출력\\nresult = llm_chain.apply(input_list)\\nfor res in result:\\n    print(res['text'].strip())\\n```\\n파리와 뉴욕의 시차는 일반적으로 6시간입니다. 파리가 뉴욕보다 6시간 앞서 있습니다. 예를 들어, 파리가 오전 9시라면 뉴욕은 오전 3시입니다.\\n서울과 하와이의 시차는 서울이 하와이보다 19시간 빠릅니다. 예를 들어, 서울이 오전 9시라면 하와이는 전날 오후 2시입니다.\\n켄버라와 베이징의 시차는 2시간입니다. 켄버라는 오스트레일리아의 수도로 UTC+10 시간대에 위치하고, 베이징은 중국의 수도로 UTC+8 시간대에 위치합니다.\\n⑤ 스트리밍(streaming)\\n스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용합니다.\\n다음과 같이 streaming=True 로 설정하고 스트리밍으로 답변을 받기 위한 StreamingStdOutCallbackHandler() 을 콜백으로 지정합니다.\\n```\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n객체 생성\\nllm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \\n                 max_tokens=2048,             # 최대 토큰수\\n                 model_name='gpt-3.5-turbo',  # 모델명\\n                 streaming=True,            \\n                 callbacks=[StreamingStdOutCallbackHandler()]\\n                )\\n```\\n```\\n질의내용\\nquestion = '대한민국의 수도는 뭐야?'\\n스트리밍으로 답변 출력\\nresponse = llm.predict(question)\\n```\\n대한민국의 수도는 서울입니다.\\n태그: ChatGPT, ChatOpenAI, GPT3.5, GPT4, langchain, langchain tutorial, OpenAI, 랭체인, 랭체인 튜토리얼\\n카테고리: langchain\\n업데이트: 2023년 09월 28일\\n공유하기\\nTwitter Facebook LinkedIn\\n이전 다음\\n댓글남기기\\n참고\\npoetry 의 거의 모든것 (튜토리얼)\\n2024년 03월 30일 5 분 소요\\nPython 개발에 있어서 poetry는 매우 강력한 도구로, 프로젝트의 의존성 관리와 패키지 배포를 간소화하는 데 큰 도움을 줍니다. 지금부터 poetry 활용 튜토리얼을 살펴 보겠습니다.\\nLangGraph Retrieval Agent를 활용한 동적 문서 검색 및 처리\\n2024년 03월 06일 10 분 소요\\nLangGraph Retrieval Agent는 언어 처리, AI 모델 통합, 데이터베이스 관리, 그래프 기반 데이터 처리 등 다양한 기능을 제공하여 언어 기반 AI 애플리케이션 개발에 필수적인 도구입니다.\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일 35 분 소요\\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일 41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 데이터 처리 작업을 수행하는 방법을 소개합니다. LangSmith 를 사용하여 Agent의 추론 단계를 추적합니다. Agent가 활용할 검색 도구(Tavily Search), PDF 기반 검색 리트리버...\\n\\n팔로우:\\nYouTube\\nGitHub\\nInstagram\\n피드\\n\\n© 2024 테디노트. Powered by Jekyll & Minimal Mistakes.\"}, {'title': '랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) - 테디노트', 'url': 'https://teddylee777.github.io/langchain/langchain-tutorial-01/', 'content': '② LangChain 한국어 튜토리얼 바로가기 👀 ③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌 ④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌 ⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌 랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) 2023년 09월 28일 5 분 소요 목차 🌱 랭체인의 주요 기능 🌱 환경설정 API KEY 발급 모듈 설치(openai, langchain) 🔥 ChatOpenAI 🔥 프롬프트 템플릿의 활용 LLMChain 객체 ① run() ② apply() ③ generate() ④ 2개 이상의 변수를 템플릿 안에 정의 ⑤ 스트리밍(streaming) 언어 모델을 활용한 애플리케이션 개발을 돕는 프레임워크인 랭체인(LangChain) 에 대해 깊이 있게 다뤄보고자 합니다. 튜토리얼은 시리즈 형식으로 구성되어, 시리즈를 거듭하면서 랭체인(LangChain) 을 통해 언어 모델 기반의 애플리케이션 개발은 더욱 간결하고 효과적으로 이루어질 수 있습니다. 추론 능력: 제공된 문맥에 기반하여 어떤 대답을 할지, 또는 어떠한 액션을 취할지에 대한 추론이 가능합니다. 특히, 이러한 사용 준비된 체인은 초보자도 랭체인을 쉽게 시작할 수 있게 도와주며, 복잡한 애플리케이션을 계획하는 전문가들은 기존 체인을 손쉽게 커스터마이징하거나 새롭게 구축할 수 있게 도와줍니다.', 'score': 0.6506676, 'raw_content': \"랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) - 테디노트\\n\\nSkip to primary navigation\\nSkip to content\\nSkip to footer\\n\\n테디노트 데이터와 인공지능을 좋아하는 개발자 노트\\n\\n검색\\n카테고리\\n태그\\n연도\\n강의\\n어바웃미\\n\\n토글 메뉴\\n\\nHome \\n/3.  Langchain \\n/5.  랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\\n\\n🔥알림🔥\\n① 테디노트 유튜브 - 구경하러 가기!\\n② LangChain 한국어 튜토리얼 바로가기 👀\\n③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌\\n④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌\\n⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌\\n랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\\n2023년 09월 28일 5 분 소요\\n목차\\n\\n🌱 랭체인의 주요 기능\\n🌱 환경설정\\nAPI KEY 발급\\n모듈 설치(openai, langchain)\\n\\n\\n🔥 ChatOpenAI\\n🔥 프롬프트 템플릿의 활용\\nLLMChain 객체\\n① run()\\n② apply()\\n③ generate()\\n④ 2개 이상의 변수를 템플릿 안에 정의\\n⑤ 스트리밍(streaming)\\n\\n\\n\\n언어 모델을 활용한 애플리케이션 개발을 돕는 프레임워크인 랭체인(LangChain) 에 대해 깊이 있게 다뤄보고자 합니다.\\n튜토리얼은 시리즈 형식으로 구성되어, 시리즈를 거듭하면서 랭체인(LangChain) 을 통해 언어 모델 기반의 애플리케이션 개발은 더욱 간결하고 효과적으로 이루어질 수 있습니다.\\n🌱 랭체인의 주요 기능\\n랭체인을 통해 다음과 같은 특징을 갖는 애플리케이션을 개발할 수 있습니다.\\n\\n문맥 인식: 언어 모델과 다양한 문맥 소스(프롬프트 지시, 예제, 응답의 근거 내용 등)를 연동하며, 사용자의 문맥을 정확히 이해합니다.\\n추론 능력: 제공된 문맥에 기반하여 어떤 대답을 할지, 또는 어떠한 액션을 취할지에 대한 추론이 가능합니다.\\n\\n랭체인의 가치\\n랭체인의 핵심적인 가치는 여러 가지가 있지만, 그 중에서도 두 가지 주요한 점을 꼽자면 다음과 같습니다.\\n\\n구성 요소: 사용자는 언어 모델과의 상호작용을 위해 다양한 구성 요소와 추상화를 활용할 수 있습니다. 이러한 구성 요소는 개별적으로, 또는 랭체인 프레임워크 내에서 모듈식으로 쉽게 활용할 수 있습니다.\\n사용 준비된 체인: 특정 고수준 작업을 수행하기 위해 미리 조립된 구성 요소의 패키지입니다.\\n\\n특히, 이러한 사용 준비된 체인은 초보자도 랭체인을 쉽게 시작할 수 있게 도와주며, 복잡한 애플리케이션을 계획하는 전문가들은 기존 체인을 손쉽게 커스터마이징하거나 새롭게 구축할 수 있게 도와줍니다.\\n🌱 환경설정\\nAPI KEY 발급\\n먼저, openai 의 API KEY 를 발급 받아야 합니다. 발급은 다음의 절차를 통해 진행할 수 있습니다.\\nhttps://platform.openai.com/account/api-keys 로 접속합니다.\\n\\nLog in 버튼을 클릭 후 계정에 로그인 합니다. 계정이 아직 생성되지 않은 경우에는 Sign up 으로 회원가입 후 로그인 합니다.\\n\\n\\n\\n“Create new secret key” 버튼을 클릭하여 새로운 키를 발급합니다.\\n\\n\\n\\nName 에는 발급하는 키에 대한 별칭을 입력합니다.\\n\\n\\n\\n새롭게 발급한 키를 복사합니다. 잃어버리면 다시 발급하여야 하므로, 안전한 곳에 저장해 둡니다.\\n\\n\\n모듈 설치(openai, langchain)\\npip 명령어로 모듈을 설치 합니다. 아나콘다 가상환경에서 설치해도 좋습니다.\\n```\\nopenai 파이썬 패키지 설치\\npip install openai langchain\\n```\\n먼저, 설치한 openai 모듈을 import 한 뒤, 발급받은 API KEY를 다음과 같이 설정합니다.\\n```\\nimport os\\nos.environ['OPENAI_API_KEY'] = 'OPENAI API KEY 입력'\\n```\\n사용 가능한 모델 리스트 출력\\n```\\nimport openai\\nmodel_list = sorted([m['id'] for m in openai.Model.list()['data']])\\nfor m in model_list:\\n    print(m)\\n```\\nada\\nada-code-search-code\\nada-code-search-text\\nada-search-document\\nada-search-query\\nada-similarity\\nbabbage\\nbabbage-002\\nbabbage-code-search-code\\nbabbage-code-search-text\\nbabbage-search-document\\nbabbage-search-query\\nbabbage-similarity\\ncode-davinci-edit-001\\ncode-search-ada-code-001\\ncode-search-ada-text-001\\ncode-search-babbage-code-001\\ncode-search-babbage-text-001\\ncurie\\ncurie-instruct-beta\\ncurie-search-document\\ncurie-search-query\\ncurie-similarity\\ndavinci\\ndavinci-002\\ndavinci-instruct-beta\\ndavinci-search-document\\ndavinci-search-query\\ndavinci-similarity\\ngpt-3.5-turbo\\ngpt-3.5-turbo-0301\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k\\ngpt-3.5-turbo-16k-0613\\ngpt-3.5-turbo-instruct\\ngpt-3.5-turbo-instruct-0914\\ngpt-4\\ngpt-4-0314\\ngpt-4-0613\\ntext-ada-001\\ntext-babbage-001\\ntext-curie-001\\ntext-davinci-001\\ntext-davinci-002\\ntext-davinci-003\\ntext-davinci-edit-001\\ntext-embedding-ada-002\\ntext-search-ada-doc-001\\ntext-search-ada-query-001\\ntext-search-babbage-doc-001\\ntext-search-babbage-query-001\\ntext-search-curie-doc-001\\ntext-search-curie-query-001\\ntext-search-davinci-doc-001\\ntext-search-davinci-query-001\\ntext-similarity-ada-001\\ntext-similarity-babbage-001\\ntext-similarity-curie-001\\ntext-similarity-davinci-001\\nwhisper-1\\n🔥 ChatOpenAI\\nOpenAI 사의 채팅 전용 Large Language Model(llm) 입니다.\\n객체를 생성할 때 다음을 옵션 값을 지정할 수 있습니다. 옵션에 대한 상세 설명은 다음과 같습니다.\\ntemperature\\n\\n사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.\\n\\nmax_tokens\\n\\n채팅 완성에서 생성할 토큰의 최대 개수입니다.\\n\\nmodel_name: 적용 가능한 모델 리스트\\n\\n\\ngpt-3.5-turbo\\n\\n\\ngpt-3.5-turbo-0301\\n\\n\\ngpt-3.5-turbo-0613\\n\\n\\ngpt-3.5-turbo-16k\\n\\n\\ngpt-3.5-turbo-16k-0613\\n\\n\\ngpt-3.5-turbo-instruct\\n\\n\\ngpt-3.5-turbo-instruct-0914\\n\\n\\ngpt-4\\n\\n\\ngpt-4-0314\\n\\n\\ngpt-4-0613\\n\\n\\n```\\nfrom langchain.chat_models import ChatOpenAI\\n객체 생성\\nllm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \\n                 max_tokens=2048,             # 최대 토큰수\\n                 model_name='gpt-3.5-turbo',  # 모델명\\n                )\\n질의내용\\nquestion = '대한민국의 수도는 뭐야?'\\n질의\\nprint(f'[답변]: {llm.predict(question)}')\\n```\\n[답변]: 대한민국의 수도는 서울입니다.\\n🔥 프롬프트 템플릿의 활용\\nPromptTemplate\\n\\n\\n사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\\n\\n\\n사용법\\n\\n\\ntemplate: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 {}는 변수를 나타냅니다.\\n\\n\\ninput_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\\n\\n\\n\\n\\ninput_variables\\n\\n\\ninput_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\\n\\n\\n사용법: 리스트 형식으로 변수 이름을 정의합니다.\\n\\n\\n```\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n질문 템플릿 형식 정의\\ntemplate = '{country}의 수도는 뭐야?'\\n템플릿 완성\\nprompt = PromptTemplate(template=template, input_variables=['country'])\\n```\\nLLMChain 객체\\nLLMChain\\n\\n\\nLLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다\\n\\n\\n사용법\\n\\n\\nprompt: 앞서 정의한 PromptTemplate 객체를 사용합니다.\\n\\n\\nllm: 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다.\\n\\n\\n\\n\\n```\\n연결된 체인(Chain)객체 생성\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n```\\n① run()\\nrun() 함수로 템플릿 프롬프트 실행\\n```\\n체인 실행: run()\\nprint(llm_chain.run(country='일본'))\\n```\\n일본의 수도는 도쿄입니다.\\n```\\n체인 실행: run()\\nprint(llm_chain.run(country='캐나다'))\\n```\\n캐나다의 수도는 오타와(Ottawa)입니다.\\n② apply()\\napply() 함수로 여러개의 입력을 한 번에 실행\\n```\\ninput_list = [\\n    {'country': '호주'},\\n    {'country': '중국'},\\n    {'country': '네덜란드'}\\n]\\nllm_chain.apply(input_list)\\n```\\n[{'text': '호주의 수도는 캔버라입니다.'},\\n {'text': '중국의 수도는 베이징(北京)입니다.'},\\n {'text': '네덜란드의 수도는 암스테르담(Amsterdam)입니다.'}]\\ntext 키 값으로 결과 뭉치가 반환되었음을 확인할 수 있습니다.\\n이를 반복문으로 출력한다면 다음과 같습니다.\\n```\\ninput_list 에 대한 결과 반환\\nresult = llm_chain.apply(input_list)\\n반복문으로 결과 출력\\nfor res in result:\\n    print(res['text'].strip())\\n```\\n호주의 수도는 캔버라입니다.\\n중국의 수도는 베이징(北京)입니다.\\n네덜란드의 수도는 암스테르담(Amsterdam)입니다.\\n③ generate()\\ngenerate() 는 문자열 대신에 LLMResult를 반환하는 점을 제외하고는 apply와 유사합니다.\\nLLMResult는 토큰 사용량과 종료 이유와 같은 유용한 생성 정보를 자주 포함하고 있습니다.\\n```\\ninput_list 에 대한 결과 반환\\ngenerated_result = llm_chain.generate(input_list)\\nprint(generated_result)\\n```\\ngenerations=[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='호주의 수도는 캔버라입니다.', additional_kwargs={}, example=False))], [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.', additional_kwargs={}, example=False))], [ChatGeneration(text='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 58, 'completion_tokens': 57, 'total_tokens': 115}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('957a5369-a20e-470a-bcea-c325b3aafb4a')), RunInfo(run_id=UUID('f5f6f639-76f8-43e3-9103-03aa7eac6fe5')), RunInfo(run_id=UUID('f9c4ce3f-4e5d-47d5-86af-f20c077b754e'))]\\n```\\n답변 출력\\ngenerated_result.generations\\n```\\n[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='호주의 수도는 캔버라입니다.', additional_kwargs={}, example=False))],\\n [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.', additional_kwargs={}, example=False))],\\n [ChatGeneration(text='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', additional_kwargs={}, example=False))]]\\n```\\n토큰 사용량 출력\\ngenerated_result.llm_output\\n```\\n{'token_usage': {'prompt_tokens': 58,\\n  'completion_tokens': 57,\\n  'total_tokens': 115},\\n 'model_name': 'gpt-3.5-turbo'}\\n```\\nrun ID 출력\\ngenerated_result.run\\n```\\n[RunInfo(run_id=UUID('957a5369-a20e-470a-bcea-c325b3aafb4a')),\\n RunInfo(run_id=UUID('f5f6f639-76f8-43e3-9103-03aa7eac6fe5')),\\n RunInfo(run_id=UUID('f9c4ce3f-4e5d-47d5-86af-f20c077b754e'))]\\n```\\n답변 출력\\nfor gen in generated_result.generations:\\n    print(gen[0].text.strip())\\n```\\n호주의 수도는 캔버라입니다.\\n중국의 수도는 베이징(北京)입니다.\\n네덜란드의 수도는 암스테르담(Amsterdam)입니다.\\n④ 2개 이상의 변수를 템플릿 안에 정의\\n2개 이상의 변수를 적용하여 템플릿을 생성할 수 있습니다.\\n이번에는 2개 이상의 변수(input_variables) 를 활용하여 템플릿 구성을 해보겠습니다.\\n```\\n질문 템플릿 형식 정의\\ntemplate = '{area1} 와 {area2} 의 시차는 몇시간이야?'\\n템플릿 완성\\nprompt = PromptTemplate(template=template, input_variables=['area1', 'area2'])\\n연결된 체인(Chain)객체 생성\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n```\\n```\\n체인 실행: run()\\nprint(llm_chain.run(area1='서울', area2='파리'))\\n```\\n서울과 파리의 시차는 8시간입니다. 서울이 파리보다 8시간 앞서 있습니다.\\n```\\ninput_list = [\\n    {'area1': '파리', 'area2': '뉴욕'},\\n    {'area1': '서울', 'area2': '하와이'},\\n    {'area1': '켄버라', 'area2': '베이징'}\\n]\\n반복문으로 결과 출력\\nresult = llm_chain.apply(input_list)\\nfor res in result:\\n    print(res['text'].strip())\\n```\\n파리와 뉴욕의 시차는 일반적으로 6시간입니다. 파리가 뉴욕보다 6시간 앞서 있습니다. 예를 들어, 파리가 오전 9시라면 뉴욕은 오전 3시입니다.\\n서울과 하와이의 시차는 서울이 하와이보다 19시간 빠릅니다. 예를 들어, 서울이 오전 9시라면 하와이는 전날 오후 2시입니다.\\n켄버라와 베이징의 시차는 2시간입니다. 켄버라는 오스트레일리아의 수도로 UTC+10 시간대에 위치하고, 베이징은 중국의 수도로 UTC+8 시간대에 위치합니다.\\n⑤ 스트리밍(streaming)\\n스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용합니다.\\n다음과 같이 streaming=True 로 설정하고 스트리밍으로 답변을 받기 위한 StreamingStdOutCallbackHandler() 을 콜백으로 지정합니다.\\n```\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n객체 생성\\nllm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \\n                 max_tokens=2048,             # 최대 토큰수\\n                 model_name='gpt-3.5-turbo',  # 모델명\\n                 streaming=True,            \\n                 callbacks=[StreamingStdOutCallbackHandler()]\\n                )\\n```\\n```\\n질의내용\\nquestion = '대한민국의 수도는 뭐야?'\\n스트리밍으로 답변 출력\\nresponse = llm.predict(question)\\n```\\n대한민국의 수도는 서울입니다.\\n태그: ChatGPT, ChatOpenAI, GPT3.5, GPT4, langchain, langchain tutorial, OpenAI, 랭체인, 랭체인 튜토리얼\\n카테고리: langchain\\n업데이트: 2023년 09월 28일\\n공유하기\\nTwitter Facebook LinkedIn\\n이전 다음\\n댓글남기기\\n참고\\npoetry 의 거의 모든것 (튜토리얼)\\n2024년 03월 30일 5 분 소요\\nPython 개발에 있어서 poetry는 매우 강력한 도구로, 프로젝트의 의존성 관리와 패키지 배포를 간소화하는 데 큰 도움을 줍니다. 지금부터 poetry 활용 튜토리얼을 살펴 보겠습니다.\\nLangGraph Retrieval Agent를 활용한 동적 문서 검색 및 처리\\n2024년 03월 06일 10 분 소요\\nLangGraph Retrieval Agent는 언어 처리, AI 모델 통합, 데이터베이스 관리, 그래프 기반 데이터 처리 등 다양한 기능을 제공하여 언어 기반 AI 애플리케이션 개발에 필수적인 도구입니다.\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일 35 분 소요\\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일 41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 데이터 처리 작업을 수행하는 방법을 소개합니다. LangSmith 를 사용하여 Agent의 추론 단계를 추적합니다. Agent가 활용할 검색 도구(Tavily Search), PDF 기반 검색 리트리버...\\n\\n팔로우:\\nYouTube\\nGitHub\\nInstagram\\n피드\\n\\n© 2024 테디노트. Powered by Jekyll & Minimal Mistakes.\"}, {'title': ' - LangChain 한국어 튜토리얼 - WikiDocs', 'url': 'https://wikidocs.net/book/14314', 'content': \"대화내용을 기억하는 RAG 체인 CH13 LangChain Expression Language(LCEL) 01. 구조화된 출력 체인(with_structered_output) CH15 평가(Evaluations) 01. 온라인 평가를 활용한 평가 자동화 CH16 에이전트(Agent) 01. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools) CH17 LangGraph 01. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기 - shcheon99@naver.com, Jan. 9, 2025, 12:28 p.m. 출력된 결과를 비교했을 때, kiwi tokenizer을 사용한 결과와 kkma, okt 를 사용한 결과가 큰 차이가 없다고 봐도 되는 건가요? CH01 LangChain 시작하기 - NamHyeon, Dec. 8, 2024, 1:17 p.m. 좋은 자료를 무료로 공유해 주셔서, 감사한 마음에 '테디노트의 RAG 비법노트' 강의 등록했습니다 ! 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory) - Jan. 16, 2025, 12:23 a.m. 멀티 에이전트 감독자(Multi-Agent Supervisor) - Dec. 23, 2024, 3:04 a.m. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams) - Dec. 23, 2024, 3:04 a.m.\", 'score': 0.5996498, 'raw_content': '<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 - WikiDocs\\n<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 CH01 LangChain 시작하기 01. 설치 영상보고 따라하기 02. OpenAI API 키 발급 및 테스트 03. LangSmith 추적 설정 04. OpenAI API 사용(GPT-4o 멀티모달) 05. LangChain Expression Language(LCEL) 06. LCEL 인터페이스 07. Runnable CH02 프롬프트(Prompt) 01. 프롬프트(Prompt) 02. 퓨샷 프롬프트(FewShotPromptTemplate) 03. LangChain Hub 04. 개인화된 프롬프트(Hub에 업로드) CH03 출력 파서(Output Parsers) 01. Pydantic 출력 파서(PydanticOutputParser) 02. 콤마 구분자 출력 파서(CommaSeparatedListOutputParser) 03. 구조화된 출력 파서(StructuredOuputParser) 04. JSON 출력 파서(JsonOutputParser) 05. 데이터프레임 출력 파서(PandasDataFrameOutputParser) 06. 날짜 형식 출력 파서(DatetimeOutputParser) 07. 열거형 출력 파서(EnumOutputParser) 08. 출력 수정 파서(OutputFixingParser) CH04 모델(Model) 01. 다양한 LLM 모델 활용 02. 캐싱(Cache) 03. 모델 직렬화(Serialization) - 저장 및 불러오기 04. 토큰 사용량 확인 05. 구글 생성 AI(Google Generative AI) 06. 허깅페이스 엔드포인트(HuggingFace Endpoints) 07. 허깅페이스 로컬(HuggingFace Local) 08. 허깅페이스 파이프라인(HuggingFace Pipeline) 09. 올라마(Ollama) 10. GPT4ALL 11. 비디오(Video) 질의 응답 LLM (Gemini) CH05 메모리(Memory) 01. 대화 버퍼 메모리(ConversationBufferMemory) 02. 대화 버퍼 윈도우 메모리(ConversationBufferWindowMemory) 03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory) 04. 대화 엔티티 메모리(ConversationEntityMemory) 05. 대화 지식그래프 메모리(ConversationKGMemory) 06. 대화 요약 메모리(ConversationSummaryMemory) 07. 벡터저장소 검색 메모리(VectorStoreRetrieverMemory) 08. LCEL Chain 에 메모리 추가 09. SQLite 에 대화내용 저장 10. RunnableWithMessageHistory에 ChatMessageHistory추가 CH06 문서 로더(Document Loader) 01. 도큐먼트(Document) 의 구조 02. PDF 03. 한글(HWP) 04. CSV 05. Excel 06. Word 07. PowerPoint 08. 웹 문서(WebBaseLoader) 09. 텍스트(TextLoader) 10. JSON 11. Arxiv 12. UpstageLayoutAnalysisLoader 13. LlamaParser CH07 텍스트 분할(Text Splitter) 01. 문자 텍스트 분할(CharacterTextSplitter) 02. 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter) 03. 토큰 텍스트 분할(TokenTextSplitter) 04. 시멘틱 청커(SemanticChunker) 05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등) 06. 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter) 07. HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter) 08. 재귀적 JSON 분할(RecursiveJsonSplitter) CH08 임베딩(Embedding) 01. OpenAIEmbeddings 02. 캐시 임베딩(CacheBackedEmbeddings) 03. 허깅페이스 임베딩(HuggingFace Embeddings) 04. UpstageEmbeddings 05. OllamaEmbeddings 06. GPT4ALL 임베딩 07. Llama CPP 임베딩 CH09 벡터저장소(VectorStore) 01. Chroma 02. FAISS 03. Pinecone CH10 검색기(Retriever) 01. 벡터스토어 기반 검색기(VectorStore-backed Retriever) 02. 문맥 압축 검색기(ContextualCompressionRetriever) 03. 앙상블 검색기(EnsembleRetriever) 04. 긴 문맥 재정렬(LongContextReorder) 05. 상위 문서 검색기(ParentDocumentRetriever) 06. 다중 쿼리 검색기(MultiQueryRetriever) 07. 다중 벡터저장소 검색기(MultiVectorRetriever) 08. 셀프 쿼리 검색기(SelfQueryRetriever) 09. 시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever) 10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기 11. Convex Combination(CC) 적용된 앙상블 검색기(EnsembleRetriever) CH11 리랭커(Reranker) 01. Cross Encoder Reranker 02. Cohere Reranker 03. Jina Reranker 04. FlashRank Reranker CH12 Retrieval Augmented Generation(RAG) 01. PDF 문서 기반 QA(Question-Answer) 02. 네이버 뉴스기사 QA(Question-Answer) 03. RAG 의 기능별 다양한 모듈 활용기 04. RAPTOR: 긴 문맥 요약(Long Context Summary) 05. 대화내용을 기억하는 RAG 체인 CH13 LangChain Expression Language(LCEL) 01. RunnablePassthrough 02. Runnable 구조(그래프) 검토 03. RunnableLambda 04. LLM 체인 라우팅(RunnableLambda, RunnableBranch) 05. RunnableParallel 06. 동적 속성 지정(configurable_fields, configurable_alternatives) 07. @chain 데코레이터로 Runnable 구성 08. RunnableWithMessageHistory 09. 사용자 정의 제네레이터(generator) 10. Runtime Arguments 바인딩 11. 폴백(fallback) 모델 지정 CH14 체인(Chains) 01. 문서 요약 02. SQL 03. 구조화된 출력 체인(with_structered_output) CH15 평가(Evaluations) 01. 합성 테스트 데이터셋 생성(RAGAS) 02. RAGAS 를 활용한 평가 03. 생성한 평가용 데이터셋 업로드(HuggingFace Dataset) 04. LangSmith 데이터셋 생성 05. LLM-as-Judge 06. 임베딩 기반 평가(embedding_distance) 07. 사용자 정의(Custom) LLM 평가 08. Rouge, BLEU, METEOR, SemScore 기반 휴리스틱 평가 09. 실험(Experiment) 평가 비교 10. 요약(Summary) 방식의 평가 11. Groundedness(할루시네이션) 평가 12. 실험 비교(Pairwise Evaluation) 13. 반복 평가 14. 온라인 평가를 활용한 평가 자동화 CH16 에이전트(Agent) 01. 도구(Tools) 02. 도구 바인딩(Binding Tools) 03. 에이전트(Agent) 04. Claude, Gemini, Ollama, Together.ai 를 활용한 Agent 05. Iteration 기능과 사람 개입(Human-in-the-loop) 06. Agentic RAG 07. CSVExcel 데이터 분석 Agent 08. Toolkits 활용 Agent 09. RAG + Image Generator Agent(보고서 작성) 10. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools) CH17 LangGraph 01. 핵심 기능 01. LangGraph 에 자주 등장하는 Python 문법이해 02. LangGraph를 활용한 챗봇 구축 03. LangGraph를 활용한 Agent 구축 04. Agent 에 메모리(memory) 추가 05. 노드의 단계별 스트리밍 출력 06. Human-in-the-loop(사람의 개입) 07. 중간단계 개입 되돌림을 통한 상태 수정과 Replay 08. 사람(Human)에게 물어보는 노드 추가 09. 메시지 삭제(RemoveMessage) 10. ToolNode 를 사용하여 도구를 호출하는 방법 11. 병렬 노드 실행을 위한 분기 생성 방법 12. 대화 기록 요약을 추가하는 방법 13. 서브그래프 추가 및 사용 방법 14. 서브그래프의 입력과 출력을 변환하는 방법 15. LangGraph 스트리밍 모드의 모든 것 02. 구조 설계 01. 기본 그래프 생성 02. Naive RAG 03. 관련성 체커(Relevance Checker) 모듈 추가 04. 웹 검색 모듈 추가 05. 쿼리 재작성 모듈 추가 06. Agentic RAG 07. Adaptive RAG 03. Use Cases 01. 에이전트 대화 시뮬레이션 (고객 응대 시나리오) 02. 사용자 요구사항 기반 메타 프롬프트 생성 에이전트 03. CRAG(Corrective RAG) 04. Self-RAG 05. 계획 후 실행(Plan-and-Execute) 06. 멀티 에이전트 협업 네트워크(Multi-Agent Collaboration Network) 07. 멀티 에이전트 감독자(Multi-Agent Supervisor) 08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams) 09. SQL 데이터베이스와 상호작용하는 에이전트 10. STORM 개념을 도입한 연구를 위한 멀티 에이전트 CH18 기타 정보 01. StreamEvent 타입별 정리\\nPublished with WikiDocs\\n\\n\\n<랭체인LangChain 노트> - Lang…\\n\\n\\n도서 증정 이벤트 !!\\n\\nWikiDocs\\n\\n<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷\\n\\nAuthor: 테디노트\\nLast edited by : Jan. 16, 2025, 12:23 a.m.\\nCopyright : \\n2,553 Like; \"추천\")\\n추천은 공유할 수 있는 무료 전자책을 집필하는데 정말 큰 힘이 됩니다. \"추천\" 한 번씩만 부탁 드리겠습니다🙏🙏\\n✅ 랭체인 한국어 튜토리얼 강의\\n패스트캠퍼스 - RAG 비법노트\\n✅ 랭체인 한국어 튜토리얼 코드저장소(GitHub) 📘🖥️\\nhttps://github.com/teddylee777/langchain-kr\\n✅ 유튜브 \"테디노트\" 🎥📚\\nhttps://www.youtube.com/c/@teddynote\\n✅ 데이터 분석 블로그 https://teddylee777.github.io\\n✅ 문의 teddylee777@gmail.com\\nLICENSE\\n인용 및 출처 표기\\n\\n본 저작물을 블로그, 유튜브 등 온라인 매체에 인용하여 게재할 경우, Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Korea 라이선스에 따라 반드시 출처를 명시해야 합니다.\\n\\n상업적 사용에 대한 사전 협의\\n\\n본 저작물(Wikidocs 및 관련 실습 코드 포함)을 강의, 강연 등 상업적 목적으로 활용하고자 하는 경우, 저작권자와의 사전 서면 협의가 필수적으로 요구됩니다. 해당 협의는 teddylee777@gmail.com으로 문의하여 진행하실 수 있습니다.\\n\\n본 저작물은 2024년 테디노트에 의해 작성되었습니다. \\n모든 권리는 저작권자에게 있으며, 본 저작물은 Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Korea 라이선스에 따라 배포됩니다.\\n본 저작물의 무단 전재 및 재배포를 금지하며, 전체 혹은 일부를 인용할 경우 출처를 명확히 밝혀주시기 바랍니다.\\n본 문서는 다른 문서의 내용을 참고하여 작성되었을 수 있습니다. 참고 자료는 본 문서 하단의 출처 목록에서 확인하실 수 있습니다.\\nCopyright (c) 테디노트.\\nReference\\n\\nLangChain Github\\nLangGraph Github\\nLangChain Document\\n\\nRecent Comments (8) Recent Modifications (10) RSS\\n02. 네이버 뉴스기사 QA(Question-Answer) - 김민겸, Feb. 2, 2025, 12:17 p.m.\\n\"bullet points 형식으로 정리\"에서 \"주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다.\" 라고 나오는데 이유를 알려주실 수 있나요? kmk582@naver.com\\n10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기 - shcheon99@naver.com, Jan. 9, 2025, 12:28 p.m.\\n출력된 결과를 비교했을 때, kiwi tokenizer을 사용한 결과와 kkma, okt 를 사용한 결과가 큰 차이가 없다고 봐도 되는 건가요?\\nCH01 LangChain 시작하기 - NamHyeon, Dec. 8, 2024, 1:17 p.m.\\n좋은 자료를 무료로 공유해 주셔서, 감사한 마음에 \\'테디노트의 RAG 비법노트\\' 강의 등록했습니다 ! 물론 제 현업에 필요한 기술이라서, 강의 또한 기쁜 마음에 신청했구요 ~ 정주행 해서, 창공을 날아가 보겠습니다 ^^\\n06. Word - Paul, Oct. 27, 2024, 5:38 p.m.\\npython-docx도 설치해야 할까요?\\n10. JSON - Paul, Oct. 27, 2024, 5:37 p.m.\\n!pip install jq 부분이 들어가야 할 것 같습니다.\\n02. PDF - Paul, Oct. 27, 2024, 3:29 p.m.\\n<html><head> <meta http-equiv=\"Content-Type\" content=\"text/html\"> </head><body> <span style=\"position:absolute; border: gray 1px solid; left:0px; top:50px; width:612px; height:858px;\"></span> <div style=\"position:absolute; top:50px;\"><a name=\"1\">Page 1</a></div> <div style=\"position:absolute; border 이 부분이 출력 결과가 아니라 코드인 것처럼 표시되어 있네요~\\n12. UpstageLayoutAnalysisLoader - Paul, Oct. 27, 2024, 10:59 a.m.\\n감사히 잘 참고하고 있습니다. 아주 사소한 오기이지만... 11번 Arxiv 다음에 12번이 와야 할 텐데, 원래 넣으시려던 다른 목차가 빠진 것인지 바로 13번이 나왔네요^^\\n03. 모델 직렬화(Serialization) - 저장 및 불러오기 - 동구, Sept. 20, 2024, 12:58 p.m.\\nloads는 뭐에요?\\n10. JSON - Jan. 16, 2025, 12:23 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory) - Jan. 16, 2025, 12:23 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등) - Jan. 16, 2025, 12:19 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n04. Self-RAG - Dec. 23, 2024, 3:48 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n10. STORM 개념을 도입한 연구를 위한 멀티 에이전트 - Dec. 23, 2024, 3:16 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n03. CRAG(Corrective RAG) - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n05. 계획 후 실행(Plan-and-Execute) - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n07. 멀티 에이전트 감독자(Multi-Agent Supervisor) - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams) - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n09. SQL 데이터베이스와 상호작용하는 에이전트 - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n\\nNext : CH01 LangChain 시작하기\\n\\n\\n×\\n책갈피\\n추가 닫기\\n\\n×\\nLeave feedback on this page\\nEmail address to reply to\\nWhat you want to say\\n※ Feedback is delivered to the author by email.\\nClose Send'}, {'title': ' - LangChain 한국어 튜토리얼 - WikiDocs', 'url': 'https://wikidocs.net/book/14314', 'content': \"대화내용을 기억하는 RAG 체인 CH13 LangChain Expression Language(LCEL) 01. 구조화된 출력 체인(with_structered_output) CH15 평가(Evaluations) 01. 온라인 평가를 활용한 평가 자동화 CH16 에이전트(Agent) 01. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools) CH17 LangGraph 01. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기 - shcheon99@naver.com, Jan. 9, 2025, 12:28 p.m. 출력된 결과를 비교했을 때, kiwi tokenizer을 사용한 결과와 kkma, okt 를 사용한 결과가 큰 차이가 없다고 봐도 되는 건가요? CH01 LangChain 시작하기 - NamHyeon, Dec. 8, 2024, 1:17 p.m. 좋은 자료를 무료로 공유해 주셔서, 감사한 마음에 '테디노트의 RAG 비법노트' 강의 등록했습니다 ! 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory) - Jan. 16, 2025, 12:23 a.m. 멀티 에이전트 감독자(Multi-Agent Supervisor) - Dec. 23, 2024, 3:04 a.m. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams) - Dec. 23, 2024, 3:04 a.m.\", 'score': 0.5996498, 'raw_content': '<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 - WikiDocs\\n<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 CH01 LangChain 시작하기 01. 설치 영상보고 따라하기 02. OpenAI API 키 발급 및 테스트 03. LangSmith 추적 설정 04. OpenAI API 사용(GPT-4o 멀티모달) 05. LangChain Expression Language(LCEL) 06. LCEL 인터페이스 07. Runnable CH02 프롬프트(Prompt) 01. 프롬프트(Prompt) 02. 퓨샷 프롬프트(FewShotPromptTemplate) 03. LangChain Hub 04. 개인화된 프롬프트(Hub에 업로드) CH03 출력 파서(Output Parsers) 01. Pydantic 출력 파서(PydanticOutputParser) 02. 콤마 구분자 출력 파서(CommaSeparatedListOutputParser) 03. 구조화된 출력 파서(StructuredOuputParser) 04. JSON 출력 파서(JsonOutputParser) 05. 데이터프레임 출력 파서(PandasDataFrameOutputParser) 06. 날짜 형식 출력 파서(DatetimeOutputParser) 07. 열거형 출력 파서(EnumOutputParser) 08. 출력 수정 파서(OutputFixingParser) CH04 모델(Model) 01. 다양한 LLM 모델 활용 02. 캐싱(Cache) 03. 모델 직렬화(Serialization) - 저장 및 불러오기 04. 토큰 사용량 확인 05. 구글 생성 AI(Google Generative AI) 06. 허깅페이스 엔드포인트(HuggingFace Endpoints) 07. 허깅페이스 로컬(HuggingFace Local) 08. 허깅페이스 파이프라인(HuggingFace Pipeline) 09. 올라마(Ollama) 10. GPT4ALL 11. 비디오(Video) 질의 응답 LLM (Gemini) CH05 메모리(Memory) 01. 대화 버퍼 메모리(ConversationBufferMemory) 02. 대화 버퍼 윈도우 메모리(ConversationBufferWindowMemory) 03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory) 04. 대화 엔티티 메모리(ConversationEntityMemory) 05. 대화 지식그래프 메모리(ConversationKGMemory) 06. 대화 요약 메모리(ConversationSummaryMemory) 07. 벡터저장소 검색 메모리(VectorStoreRetrieverMemory) 08. LCEL Chain 에 메모리 추가 09. SQLite 에 대화내용 저장 10. RunnableWithMessageHistory에 ChatMessageHistory추가 CH06 문서 로더(Document Loader) 01. 도큐먼트(Document) 의 구조 02. PDF 03. 한글(HWP) 04. CSV 05. Excel 06. Word 07. PowerPoint 08. 웹 문서(WebBaseLoader) 09. 텍스트(TextLoader) 10. JSON 11. Arxiv 12. UpstageLayoutAnalysisLoader 13. LlamaParser CH07 텍스트 분할(Text Splitter) 01. 문자 텍스트 분할(CharacterTextSplitter) 02. 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter) 03. 토큰 텍스트 분할(TokenTextSplitter) 04. 시멘틱 청커(SemanticChunker) 05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등) 06. 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter) 07. HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter) 08. 재귀적 JSON 분할(RecursiveJsonSplitter) CH08 임베딩(Embedding) 01. OpenAIEmbeddings 02. 캐시 임베딩(CacheBackedEmbeddings) 03. 허깅페이스 임베딩(HuggingFace Embeddings) 04. UpstageEmbeddings 05. OllamaEmbeddings 06. GPT4ALL 임베딩 07. Llama CPP 임베딩 CH09 벡터저장소(VectorStore) 01. Chroma 02. FAISS 03. Pinecone CH10 검색기(Retriever) 01. 벡터스토어 기반 검색기(VectorStore-backed Retriever) 02. 문맥 압축 검색기(ContextualCompressionRetriever) 03. 앙상블 검색기(EnsembleRetriever) 04. 긴 문맥 재정렬(LongContextReorder) 05. 상위 문서 검색기(ParentDocumentRetriever) 06. 다중 쿼리 검색기(MultiQueryRetriever) 07. 다중 벡터저장소 검색기(MultiVectorRetriever) 08. 셀프 쿼리 검색기(SelfQueryRetriever) 09. 시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever) 10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기 11. Convex Combination(CC) 적용된 앙상블 검색기(EnsembleRetriever) CH11 리랭커(Reranker) 01. Cross Encoder Reranker 02. Cohere Reranker 03. Jina Reranker 04. FlashRank Reranker CH12 Retrieval Augmented Generation(RAG) 01. PDF 문서 기반 QA(Question-Answer) 02. 네이버 뉴스기사 QA(Question-Answer) 03. RAG 의 기능별 다양한 모듈 활용기 04. RAPTOR: 긴 문맥 요약(Long Context Summary) 05. 대화내용을 기억하는 RAG 체인 CH13 LangChain Expression Language(LCEL) 01. RunnablePassthrough 02. Runnable 구조(그래프) 검토 03. RunnableLambda 04. LLM 체인 라우팅(RunnableLambda, RunnableBranch) 05. RunnableParallel 06. 동적 속성 지정(configurable_fields, configurable_alternatives) 07. @chain 데코레이터로 Runnable 구성 08. RunnableWithMessageHistory 09. 사용자 정의 제네레이터(generator) 10. Runtime Arguments 바인딩 11. 폴백(fallback) 모델 지정 CH14 체인(Chains) 01. 문서 요약 02. SQL 03. 구조화된 출력 체인(with_structered_output) CH15 평가(Evaluations) 01. 합성 테스트 데이터셋 생성(RAGAS) 02. RAGAS 를 활용한 평가 03. 생성한 평가용 데이터셋 업로드(HuggingFace Dataset) 04. LangSmith 데이터셋 생성 05. LLM-as-Judge 06. 임베딩 기반 평가(embedding_distance) 07. 사용자 정의(Custom) LLM 평가 08. Rouge, BLEU, METEOR, SemScore 기반 휴리스틱 평가 09. 실험(Experiment) 평가 비교 10. 요약(Summary) 방식의 평가 11. Groundedness(할루시네이션) 평가 12. 실험 비교(Pairwise Evaluation) 13. 반복 평가 14. 온라인 평가를 활용한 평가 자동화 CH16 에이전트(Agent) 01. 도구(Tools) 02. 도구 바인딩(Binding Tools) 03. 에이전트(Agent) 04. Claude, Gemini, Ollama, Together.ai 를 활용한 Agent 05. Iteration 기능과 사람 개입(Human-in-the-loop) 06. Agentic RAG 07. CSVExcel 데이터 분석 Agent 08. Toolkits 활용 Agent 09. RAG + Image Generator Agent(보고서 작성) 10. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools) CH17 LangGraph 01. 핵심 기능 01. LangGraph 에 자주 등장하는 Python 문법이해 02. LangGraph를 활용한 챗봇 구축 03. LangGraph를 활용한 Agent 구축 04. Agent 에 메모리(memory) 추가 05. 노드의 단계별 스트리밍 출력 06. Human-in-the-loop(사람의 개입) 07. 중간단계 개입 되돌림을 통한 상태 수정과 Replay 08. 사람(Human)에게 물어보는 노드 추가 09. 메시지 삭제(RemoveMessage) 10. ToolNode 를 사용하여 도구를 호출하는 방법 11. 병렬 노드 실행을 위한 분기 생성 방법 12. 대화 기록 요약을 추가하는 방법 13. 서브그래프 추가 및 사용 방법 14. 서브그래프의 입력과 출력을 변환하는 방법 15. LangGraph 스트리밍 모드의 모든 것 02. 구조 설계 01. 기본 그래프 생성 02. Naive RAG 03. 관련성 체커(Relevance Checker) 모듈 추가 04. 웹 검색 모듈 추가 05. 쿼리 재작성 모듈 추가 06. Agentic RAG 07. Adaptive RAG 03. Use Cases 01. 에이전트 대화 시뮬레이션 (고객 응대 시나리오) 02. 사용자 요구사항 기반 메타 프롬프트 생성 에이전트 03. CRAG(Corrective RAG) 04. Self-RAG 05. 계획 후 실행(Plan-and-Execute) 06. 멀티 에이전트 협업 네트워크(Multi-Agent Collaboration Network) 07. 멀티 에이전트 감독자(Multi-Agent Supervisor) 08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams) 09. SQL 데이터베이스와 상호작용하는 에이전트 10. STORM 개념을 도입한 연구를 위한 멀티 에이전트 CH18 기타 정보 01. StreamEvent 타입별 정리\\nPublished with WikiDocs\\n\\n\\n<랭체인LangChain 노트> - Lang…\\n\\n\\n도서 증정 이벤트 !!\\n\\nWikiDocs\\n\\n<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷\\n\\nAuthor: 테디노트\\nLast edited by : Jan. 16, 2025, 12:23 a.m.\\nCopyright : \\n2,553 Like; \"추천\")\\n추천은 공유할 수 있는 무료 전자책을 집필하는데 정말 큰 힘이 됩니다. \"추천\" 한 번씩만 부탁 드리겠습니다🙏🙏\\n✅ 랭체인 한국어 튜토리얼 강의\\n패스트캠퍼스 - RAG 비법노트\\n✅ 랭체인 한국어 튜토리얼 코드저장소(GitHub) 📘🖥️\\nhttps://github.com/teddylee777/langchain-kr\\n✅ 유튜브 \"테디노트\" 🎥📚\\nhttps://www.youtube.com/c/@teddynote\\n✅ 데이터 분석 블로그 https://teddylee777.github.io\\n✅ 문의 teddylee777@gmail.com\\nLICENSE\\n인용 및 출처 표기\\n\\n본 저작물을 블로그, 유튜브 등 온라인 매체에 인용하여 게재할 경우, Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Korea 라이선스에 따라 반드시 출처를 명시해야 합니다.\\n\\n상업적 사용에 대한 사전 협의\\n\\n본 저작물(Wikidocs 및 관련 실습 코드 포함)을 강의, 강연 등 상업적 목적으로 활용하고자 하는 경우, 저작권자와의 사전 서면 협의가 필수적으로 요구됩니다. 해당 협의는 teddylee777@gmail.com으로 문의하여 진행하실 수 있습니다.\\n\\n본 저작물은 2024년 테디노트에 의해 작성되었습니다. \\n모든 권리는 저작권자에게 있으며, 본 저작물은 Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Korea 라이선스에 따라 배포됩니다.\\n본 저작물의 무단 전재 및 재배포를 금지하며, 전체 혹은 일부를 인용할 경우 출처를 명확히 밝혀주시기 바랍니다.\\n본 문서는 다른 문서의 내용을 참고하여 작성되었을 수 있습니다. 참고 자료는 본 문서 하단의 출처 목록에서 확인하실 수 있습니다.\\nCopyright (c) 테디노트.\\nReference\\n\\nLangChain Github\\nLangGraph Github\\nLangChain Document\\n\\nRecent Comments (8) Recent Modifications (10) RSS\\n02. 네이버 뉴스기사 QA(Question-Answer) - 김민겸, Feb. 2, 2025, 12:17 p.m.\\n\"bullet points 형식으로 정리\"에서 \"주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다.\" 라고 나오는데 이유를 알려주실 수 있나요? kmk582@naver.com\\n10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기 - shcheon99@naver.com, Jan. 9, 2025, 12:28 p.m.\\n출력된 결과를 비교했을 때, kiwi tokenizer을 사용한 결과와 kkma, okt 를 사용한 결과가 큰 차이가 없다고 봐도 되는 건가요?\\nCH01 LangChain 시작하기 - NamHyeon, Dec. 8, 2024, 1:17 p.m.\\n좋은 자료를 무료로 공유해 주셔서, 감사한 마음에 \\'테디노트의 RAG 비법노트\\' 강의 등록했습니다 ! 물론 제 현업에 필요한 기술이라서, 강의 또한 기쁜 마음에 신청했구요 ~ 정주행 해서, 창공을 날아가 보겠습니다 ^^\\n06. Word - Paul, Oct. 27, 2024, 5:38 p.m.\\npython-docx도 설치해야 할까요?\\n10. JSON - Paul, Oct. 27, 2024, 5:37 p.m.\\n!pip install jq 부분이 들어가야 할 것 같습니다.\\n02. PDF - Paul, Oct. 27, 2024, 3:29 p.m.\\n<html><head> <meta http-equiv=\"Content-Type\" content=\"text/html\"> </head><body> <span style=\"position:absolute; border: gray 1px solid; left:0px; top:50px; width:612px; height:858px;\"></span> <div style=\"position:absolute; top:50px;\"><a name=\"1\">Page 1</a></div> <div style=\"position:absolute; border 이 부분이 출력 결과가 아니라 코드인 것처럼 표시되어 있네요~\\n12. UpstageLayoutAnalysisLoader - Paul, Oct. 27, 2024, 10:59 a.m.\\n감사히 잘 참고하고 있습니다. 아주 사소한 오기이지만... 11번 Arxiv 다음에 12번이 와야 할 텐데, 원래 넣으시려던 다른 목차가 빠진 것인지 바로 13번이 나왔네요^^\\n03. 모델 직렬화(Serialization) - 저장 및 불러오기 - 동구, Sept. 20, 2024, 12:58 p.m.\\nloads는 뭐에요?\\n10. JSON - Jan. 16, 2025, 12:23 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory) - Jan. 16, 2025, 12:23 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등) - Jan. 16, 2025, 12:19 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n04. Self-RAG - Dec. 23, 2024, 3:48 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n10. STORM 개념을 도입한 연구를 위한 멀티 에이전트 - Dec. 23, 2024, 3:16 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n03. CRAG(Corrective RAG) - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n05. 계획 후 실행(Plan-and-Execute) - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n07. 멀티 에이전트 감독자(Multi-Agent Supervisor) - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams) - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n09. SQL 데이터베이스와 상호작용하는 에이전트 - Dec. 23, 2024, 3:04 a.m.\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\n\\nNext : CH01 LangChain 시작하기\\n\\n\\n×\\n책갈피\\n추가 닫기\\n\\n×\\nLeave feedback on this page\\nEmail address to reply to\\nWhat you want to say\\n※ Feedback is delivered to the author by email.\\nClose Send'}, {'title': '랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8) - 테디노트', 'url': 'https://teddylee777.github.io/langchain/langchain-tutorial-08/', 'content': '② LangChain 한국어 튜토리얼 바로가기 👀 ③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌 ④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌 ⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌 랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8) 2023년 10월 13일 2 분 소요 목차 🌱 환경설정 🔥 PDF 기반 질의 응답(Question-Answering) ① 데이터 로드 ② 데이터 분할 ③ 저장 및 검색 ④ 프롬프트 템플릿 ⑤ 생성 ⑥ 테스트 이번 포스팅에서는 랭체인(LangChain) 을 활용하여 PDF 문서를 로드하고, 문서의 내용에 기반하여 질의응답(Question-Answering) 하는 방법에 대해 알아보겠습니다. 이번 튜토리얼에서는 langchain 의 문서 로드 - 분할 - 벡터스토어(vectorstore)에 임베딩된 문서를 저장 하는 방법을 다룹니다. 후반부에는 langchain hub 에서 프롬프트를 다운로드 받고, 이를 ChatGPT 모델과 결합하여 문서에 기반한 질의응답 Chain 을 생성합니다. 생성: LLM은 질문과 검색된 데이터를 포함하는 프롬프트를 사용하여 답변을 생성합니다.', 'score': 0.55998856, 'raw_content': '랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8) - 테디노트\\n\\nSkip to primary navigation\\nSkip to content\\nSkip to footer\\n\\n테디노트 데이터와 인공지능을 좋아하는 개발자 노트\\n\\n검색\\n카테고리\\n태그\\n연도\\n강의\\n어바웃미\\n\\n토글 메뉴\\n\\nHome \\n/3.  Langchain \\n/5.  랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8)\\n\\n🔥알림🔥\\n① 테디노트 유튜브 - 구경하러 가기!\\n② LangChain 한국어 튜토리얼 바로가기 👀\\n③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌\\n④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌\\n⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌\\n랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8)\\n2023년 10월 13일 2 분 소요\\n목차\\n\\n🌱 환경설정\\n🔥 PDF 기반 질의 응답(Question-Answering)\\n① 데이터 로드\\n② 데이터 분할\\n③ 저장 및 검색\\n④ 프롬프트 템플릿\\n⑤ 생성\\n⑥ 테스트\\n\\n\\n\\n이번 포스팅에서는 랭체인(LangChain) 을 활용하여 PDF 문서를 로드하고, 문서의 내용에 기반하여 질의응답(Question-Answering) 하는 방법에 대해 알아보겠습니다.\\n이번 튜토리얼에서는 langchain 의 문서 로드 - 분할 - 벡터스토어(vectorstore)에 임베딩된 문서를 저장 하는 방법을 다룹니다. 여러 벡터스토어 중 오픈소스인 Chroma DB 를 활용합니다.\\n후반부에는 langchain hub 에서 프롬프트를 다운로드 받고, 이를 ChatGPT 모델과 결합하여 문서에 기반한 질의응답 Chain 을 생성합니다.\\n\\n✔️ (이전글) LangChain 튜토리얼\\n\\n랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법\\n랭체인(langchain) + 허깅페이스(HuggingFace) 모델 사용법\\n랭체인(langchain) + 챗(chat) - ConversationChain, 템플릿 사용법\\n랭체인(langchain) + 정형데이터(CSV, Excel) - ChatGPT 기반 데이터분석\\n랭체인(langchain) + 웹사이트 크롤링 - 웹사이트 문서 요약\\n랭체인(langchain) + 웹사이트 정보 추출 - 스키마 활용법\\n랭체인(langchain) + PDF 문서요약, Map-Reduce\\n\\n\\n🌱 환경설정\\n```\\n필요한 라이브러리 설치\\n!pip install -q openai langchain langchainhub pypdf\\n```\\n```\\nOPENAI_API\\nimport os\\nos.environ[\\'OPENAI_API_KEY\\'] = \\'OPENAI API KEY 입력\\'\\n```\\n```\\n토큰 정보로드를 위한 라이브러리\\n설치: pip install python-dotenv\\nfrom dotenv import load_dotenv\\n토큰 정보로드\\nload_dotenv()\\n```\\nTrue\\n🔥 PDF 기반 질의 응답(Question-Answering)\\n\\n다음은 비구조화된 데이터를 QA 체인(Question-Answering chain) 으로 변환하는 파이프라인에 대한 기술적 번역입니다:\\n\\n\\n데이터 로드: 우선, 데이터를 로드해야 합니다. LangChain 통합 허브를 사용하여 전체 로더 세트를 둘러보세요.\\n\\n\\n데이터 분할: 텍스트 분할기는 문서를 지정된 크기의 분할로 나눕니다.\\n\\n\\n저장: 저장소(예: 종종 vectorstore)는 분할을 보관하고 종종 임베드합니다.\\n\\n\\n검색: 앱은 저장소에서 분할을 검색합니다(예: 종종 입력 질문과 유사한 임베딩으로).\\n\\n\\n생성: LLM은 질문과 검색된 데이터를 포함하는 프롬프트를 사용하여 답변을 생성합니다.\\n\\n\\n① 데이터 로드\\nPyPDFLoader 를 활용하여 PDF 파일을 로드 합니다.\\n```\\nfrom langchain.document_loaders import PyPDFLoader\\nPDF 파일 로드\\nloader = PyPDFLoader(\"data/황순원-소나기.pdf\")\\ndocument = loader.load()\\ndocument[0].page_content[:200] # 내용 추출\\n```\\n\\'- 1 -소나기\\\\n황순원\\\\n소년은 개울가에서 소녀를 보자 곧 윤 초시네 증손녀 (曾孫女 )딸이라는 걸 알 수 있었다 . \\\\n소녀는 개울에다 손을 잠그고 물장난을 하고 있는 것이다 . 서울서는 이런 개울물을 보지 \\\\n못하기나 한 듯이.\\\\n벌써 며칠째 소녀는 , 학교에서 돌아오는 길에 물장난이었다 . 그런데 , 어제까지 개울 기슭에\\\\n서 하더니 , 오늘은 징검다리 한가운\\'\\n② 데이터 분할\\nCharacterTextSplitter 로 chunk_size 기준으로 문서를 쪼갭니다. chunk_overlap 에 50개의 토큰을 지정하여 문서-문서 간 겹쳐지는 부분(overlap) 이 있도록 하여 비교적 유연한 요약 결과를 도출할 수 있도록 합니다.\\n```\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\ntexts = text_splitter.split_documents(document)\\n```\\n③ 저장 및 검색\\nOpenAIEmbeddings 를 활용하여 문서의 내용을 임베딩한 뒤, Chroma 벡터스토어(vectorstore) 에 저장합니다.\\n마지막 줄에는 as_retriever() 로 retriever 형태로 가져오는데, 이는 추후 사용자의 query 입력시, 입력된 query로 vectorestore에서 유사성이 높은 데이터를 추출해 낼 때 쓰입니다.\\n```\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\n임베딩\\nembeddings = OpenAIEmbeddings()\\nChroma DB 에 저장\\ndocsearch = Chroma.from_documents(texts, embeddings)\\nretriever 가져옴\\nretriever = docsearch.as_retriever()\\n```\\n④ 프롬프트 템플릿\\n아래의 예제는 langchain hub 에서 RAG Prompt 를 가져오는 예제입니다.\\n이처럼 langchain hub 에서 공개된 프롬프트를 다운로드 받거나, ChatPromptTemplate 를 직접 생성하는 것도 가능합니다. 자세한 사항은 ConversationChain, 템플릿 사용법 에서 확인할 수 있습니다.\\n```\\nlangchain hub 에서 Prompt 다운로드 예시\\nhttps://smith.langchain.com/hub/rlm/rag-prompt\\nfrom langchain import hub\\nrag_prompt = hub.pull(\"rlm/rag-prompt\")\\nrag_prompt\\n```\\nChatPromptTemplate(input_variables=[\\'question\\', \\'context\\'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\\'question\\', \\'context\\'], output_parser=None, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\\\nQuestion: {question} \\\\nContext: {context} \\\\nAnswer:\", template_format=\\'f-string\\', validate_template=True), additional_kwargs={})])\\n⑤ 생성\\n마지막 단계는 LLM 모델을 정의하고 Chain 을 생성하는 단계 입니다.\\n```\\nLLM\\nfrom langchain.chat_models import ChatOpenAI\\nChatGPT 모델 지정\\nllm = ChatOpenAI(model_name=\"gpt-4-0613\", temperature=0)\\n```\\n```\\nRAG chain 생성\\nfrom langchain.schema.runnable import RunnablePassthrough\\npipe operator를 활용한 체인 생성\\nrag_chain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | rag_prompt \\n    | llm \\n)\\n```\\n⑥ 테스트\\nrag_chain.invoke(\"이 소설의 제목은 뭐야?\")\\nAIMessage(content=\\'이 소설의 제목은 \"소나기\"입니다.\\', additional_kwargs={}, example=False)\\nrag_chain.invoke(\"이 소설의 저자는 누구야?\")\\nAIMessage(content=\\'이 소설의 저자는 황순원입니다.\\', additional_kwargs={}, example=False)\\n태그: ChatGPT, ChatOpenAI, GPT3.5, GPT4, langchain, langchain tutorial, OpenAI, PDF, 랭체인, 랭체인 튜토리얼, 문서요약, 질의응답, 크롤링\\n카테고리: langchain\\n업데이트: 2023년 10월 13일\\n공유하기\\nTwitter Facebook LinkedIn\\n이전 다음\\n댓글남기기\\n참고\\npoetry 의 거의 모든것 (튜토리얼)\\n2024년 03월 30일 5 분 소요\\nPython 개발에 있어서 poetry는 매우 강력한 도구로, 프로젝트의 의존성 관리와 패키지 배포를 간소화하는 데 큰 도움을 줍니다. 지금부터 poetry 활용 튜토리얼을 살펴 보겠습니다.\\nLangGraph Retrieval Agent를 활용한 동적 문서 검색 및 처리\\n2024년 03월 06일 10 분 소요\\nLangGraph Retrieval Agent는 언어 처리, AI 모델 통합, 데이터베이스 관리, 그래프 기반 데이터 처리 등 다양한 기능을 제공하여 언어 기반 AI 애플리케이션 개발에 필수적인 도구입니다.\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일 35 분 소요\\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일 41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 데이터 처리 작업을 수행하는 방법을 소개합니다. LangSmith 를 사용하여 Agent의 추론 단계를 추적합니다. Agent가 활용할 검색 도구(Tavily Search), PDF 기반 검색 리트리버...\\n\\n팔로우:\\nYouTube\\nGitHub\\nInstagram\\n피드\\n\\n© 2024 테디노트. Powered by Jekyll & Minimal Mistakes.'}, {'title': '랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8) - 테디노트', 'url': 'https://teddylee777.github.io/langchain/langchain-tutorial-08/', 'content': '② LangChain 한국어 튜토리얼 바로가기 👀 ③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌 ④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌 ⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌 랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8) 2023년 10월 13일 2 분 소요 목차 🌱 환경설정 🔥 PDF 기반 질의 응답(Question-Answering) ① 데이터 로드 ② 데이터 분할 ③ 저장 및 검색 ④ 프롬프트 템플릿 ⑤ 생성 ⑥ 테스트 이번 포스팅에서는 랭체인(LangChain) 을 활용하여 PDF 문서를 로드하고, 문서의 내용에 기반하여 질의응답(Question-Answering) 하는 방법에 대해 알아보겠습니다. 이번 튜토리얼에서는 langchain 의 문서 로드 - 분할 - 벡터스토어(vectorstore)에 임베딩된 문서를 저장 하는 방법을 다룹니다. 후반부에는 langchain hub 에서 프롬프트를 다운로드 받고, 이를 ChatGPT 모델과 결합하여 문서에 기반한 질의응답 Chain 을 생성합니다. 생성: LLM은 질문과 검색된 데이터를 포함하는 프롬프트를 사용하여 답변을 생성합니다.', 'score': 0.55998856, 'raw_content': '랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8) - 테디노트\\n\\nSkip to primary navigation\\nSkip to content\\nSkip to footer\\n\\n테디노트 데이터와 인공지능을 좋아하는 개발자 노트\\n\\n검색\\n카테고리\\n태그\\n연도\\n강의\\n어바웃미\\n\\n토글 메뉴\\n\\nHome \\n/3.  Langchain \\n/5.  랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8)\\n\\n🔥알림🔥\\n① 테디노트 유튜브 - 구경하러 가기!\\n② LangChain 한국어 튜토리얼 바로가기 👀\\n③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌\\n④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌\\n⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌\\n랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8)\\n2023년 10월 13일 2 분 소요\\n목차\\n\\n🌱 환경설정\\n🔥 PDF 기반 질의 응답(Question-Answering)\\n① 데이터 로드\\n② 데이터 분할\\n③ 저장 및 검색\\n④ 프롬프트 템플릿\\n⑤ 생성\\n⑥ 테스트\\n\\n\\n\\n이번 포스팅에서는 랭체인(LangChain) 을 활용하여 PDF 문서를 로드하고, 문서의 내용에 기반하여 질의응답(Question-Answering) 하는 방법에 대해 알아보겠습니다.\\n이번 튜토리얼에서는 langchain 의 문서 로드 - 분할 - 벡터스토어(vectorstore)에 임베딩된 문서를 저장 하는 방법을 다룹니다. 여러 벡터스토어 중 오픈소스인 Chroma DB 를 활용합니다.\\n후반부에는 langchain hub 에서 프롬프트를 다운로드 받고, 이를 ChatGPT 모델과 결합하여 문서에 기반한 질의응답 Chain 을 생성합니다.\\n\\n✔️ (이전글) LangChain 튜토리얼\\n\\n랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법\\n랭체인(langchain) + 허깅페이스(HuggingFace) 모델 사용법\\n랭체인(langchain) + 챗(chat) - ConversationChain, 템플릿 사용법\\n랭체인(langchain) + 정형데이터(CSV, Excel) - ChatGPT 기반 데이터분석\\n랭체인(langchain) + 웹사이트 크롤링 - 웹사이트 문서 요약\\n랭체인(langchain) + 웹사이트 정보 추출 - 스키마 활용법\\n랭체인(langchain) + PDF 문서요약, Map-Reduce\\n\\n\\n🌱 환경설정\\n```\\n필요한 라이브러리 설치\\n!pip install -q openai langchain langchainhub pypdf\\n```\\n```\\nOPENAI_API\\nimport os\\nos.environ[\\'OPENAI_API_KEY\\'] = \\'OPENAI API KEY 입력\\'\\n```\\n```\\n토큰 정보로드를 위한 라이브러리\\n설치: pip install python-dotenv\\nfrom dotenv import load_dotenv\\n토큰 정보로드\\nload_dotenv()\\n```\\nTrue\\n🔥 PDF 기반 질의 응답(Question-Answering)\\n\\n다음은 비구조화된 데이터를 QA 체인(Question-Answering chain) 으로 변환하는 파이프라인에 대한 기술적 번역입니다:\\n\\n\\n데이터 로드: 우선, 데이터를 로드해야 합니다. LangChain 통합 허브를 사용하여 전체 로더 세트를 둘러보세요.\\n\\n\\n데이터 분할: 텍스트 분할기는 문서를 지정된 크기의 분할로 나눕니다.\\n\\n\\n저장: 저장소(예: 종종 vectorstore)는 분할을 보관하고 종종 임베드합니다.\\n\\n\\n검색: 앱은 저장소에서 분할을 검색합니다(예: 종종 입력 질문과 유사한 임베딩으로).\\n\\n\\n생성: LLM은 질문과 검색된 데이터를 포함하는 프롬프트를 사용하여 답변을 생성합니다.\\n\\n\\n① 데이터 로드\\nPyPDFLoader 를 활용하여 PDF 파일을 로드 합니다.\\n```\\nfrom langchain.document_loaders import PyPDFLoader\\nPDF 파일 로드\\nloader = PyPDFLoader(\"data/황순원-소나기.pdf\")\\ndocument = loader.load()\\ndocument[0].page_content[:200] # 내용 추출\\n```\\n\\'- 1 -소나기\\\\n황순원\\\\n소년은 개울가에서 소녀를 보자 곧 윤 초시네 증손녀 (曾孫女 )딸이라는 걸 알 수 있었다 . \\\\n소녀는 개울에다 손을 잠그고 물장난을 하고 있는 것이다 . 서울서는 이런 개울물을 보지 \\\\n못하기나 한 듯이.\\\\n벌써 며칠째 소녀는 , 학교에서 돌아오는 길에 물장난이었다 . 그런데 , 어제까지 개울 기슭에\\\\n서 하더니 , 오늘은 징검다리 한가운\\'\\n② 데이터 분할\\nCharacterTextSplitter 로 chunk_size 기준으로 문서를 쪼갭니다. chunk_overlap 에 50개의 토큰을 지정하여 문서-문서 간 겹쳐지는 부분(overlap) 이 있도록 하여 비교적 유연한 요약 결과를 도출할 수 있도록 합니다.\\n```\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\ntexts = text_splitter.split_documents(document)\\n```\\n③ 저장 및 검색\\nOpenAIEmbeddings 를 활용하여 문서의 내용을 임베딩한 뒤, Chroma 벡터스토어(vectorstore) 에 저장합니다.\\n마지막 줄에는 as_retriever() 로 retriever 형태로 가져오는데, 이는 추후 사용자의 query 입력시, 입력된 query로 vectorestore에서 유사성이 높은 데이터를 추출해 낼 때 쓰입니다.\\n```\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\n임베딩\\nembeddings = OpenAIEmbeddings()\\nChroma DB 에 저장\\ndocsearch = Chroma.from_documents(texts, embeddings)\\nretriever 가져옴\\nretriever = docsearch.as_retriever()\\n```\\n④ 프롬프트 템플릿\\n아래의 예제는 langchain hub 에서 RAG Prompt 를 가져오는 예제입니다.\\n이처럼 langchain hub 에서 공개된 프롬프트를 다운로드 받거나, ChatPromptTemplate 를 직접 생성하는 것도 가능합니다. 자세한 사항은 ConversationChain, 템플릿 사용법 에서 확인할 수 있습니다.\\n```\\nlangchain hub 에서 Prompt 다운로드 예시\\nhttps://smith.langchain.com/hub/rlm/rag-prompt\\nfrom langchain import hub\\nrag_prompt = hub.pull(\"rlm/rag-prompt\")\\nrag_prompt\\n```\\nChatPromptTemplate(input_variables=[\\'question\\', \\'context\\'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\\'question\\', \\'context\\'], output_parser=None, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\\\nQuestion: {question} \\\\nContext: {context} \\\\nAnswer:\", template_format=\\'f-string\\', validate_template=True), additional_kwargs={})])\\n⑤ 생성\\n마지막 단계는 LLM 모델을 정의하고 Chain 을 생성하는 단계 입니다.\\n```\\nLLM\\nfrom langchain.chat_models import ChatOpenAI\\nChatGPT 모델 지정\\nllm = ChatOpenAI(model_name=\"gpt-4-0613\", temperature=0)\\n```\\n```\\nRAG chain 생성\\nfrom langchain.schema.runnable import RunnablePassthrough\\npipe operator를 활용한 체인 생성\\nrag_chain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | rag_prompt \\n    | llm \\n)\\n```\\n⑥ 테스트\\nrag_chain.invoke(\"이 소설의 제목은 뭐야?\")\\nAIMessage(content=\\'이 소설의 제목은 \"소나기\"입니다.\\', additional_kwargs={}, example=False)\\nrag_chain.invoke(\"이 소설의 저자는 누구야?\")\\nAIMessage(content=\\'이 소설의 저자는 황순원입니다.\\', additional_kwargs={}, example=False)\\n태그: ChatGPT, ChatOpenAI, GPT3.5, GPT4, langchain, langchain tutorial, OpenAI, PDF, 랭체인, 랭체인 튜토리얼, 문서요약, 질의응답, 크롤링\\n카테고리: langchain\\n업데이트: 2023년 10월 13일\\n공유하기\\nTwitter Facebook LinkedIn\\n이전 다음\\n댓글남기기\\n참고\\npoetry 의 거의 모든것 (튜토리얼)\\n2024년 03월 30일 5 분 소요\\nPython 개발에 있어서 poetry는 매우 강력한 도구로, 프로젝트의 의존성 관리와 패키지 배포를 간소화하는 데 큰 도움을 줍니다. 지금부터 poetry 활용 튜토리얼을 살펴 보겠습니다.\\nLangGraph Retrieval Agent를 활용한 동적 문서 검색 및 처리\\n2024년 03월 06일 10 분 소요\\nLangGraph Retrieval Agent는 언어 처리, AI 모델 통합, 데이터베이스 관리, 그래프 기반 데이터 처리 등 다양한 기능을 제공하여 언어 기반 AI 애플리케이션 개발에 필수적인 도구입니다.\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일 35 분 소요\\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일 41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 데이터 처리 작업을 수행하는 방법을 소개합니다. LangSmith 를 사용하여 Agent의 추론 단계를 추적합니다. Agent가 활용할 검색 도구(Tavily Search), PDF 기반 검색 리트리버...\\n\\n팔로우:\\nYouTube\\nGitHub\\nInstagram\\n피드\\n\\n© 2024 테디노트. Powered by Jekyll & Minimal Mistakes.'}]\n"
     ]
    }
   ],
   "source": [
    "# 웹 검색 도구 호출\n",
    "result = web_search_tool.search(\"테디노트 위키독스 랭체인 튜토리얼 URL 을 알려주세요\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1904c95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) - 테디노트',\n",
       " 'url': 'https://teddylee777.github.io/langchain/langchain-tutorial-01/',\n",
       " 'content': '② LangChain 한국어 튜토리얼 바로가기 👀 ③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌 ④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌 ⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌 랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) 2023년 09월 28일 5 분 소요 목차 🌱 랭체인의 주요 기능 🌱 환경설정 API KEY 발급 모듈 설치(openai, langchain) 🔥 ChatOpenAI 🔥 프롬프트 템플릿의 활용 LLMChain 객체 ① run() ② apply() ③ generate() ④ 2개 이상의 변수를 템플릿 안에 정의 ⑤ 스트리밍(streaming) 언어 모델을 활용한 애플리케이션 개발을 돕는 프레임워크인 랭체인(LangChain) 에 대해 깊이 있게 다뤄보고자 합니다. 튜토리얼은 시리즈 형식으로 구성되어, 시리즈를 거듭하면서 랭체인(LangChain) 을 통해 언어 모델 기반의 애플리케이션 개발은 더욱 간결하고 효과적으로 이루어질 수 있습니다. 추론 능력: 제공된 문맥에 기반하여 어떤 대답을 할지, 또는 어떠한 액션을 취할지에 대한 추론이 가능합니다. 특히, 이러한 사용 준비된 체인은 초보자도 랭체인을 쉽게 시작할 수 있게 도와주며, 복잡한 애플리케이션을 계획하는 전문가들은 기존 체인을 손쉽게 커스터마이징하거나 새롭게 구축할 수 있게 도와줍니다.',\n",
       " 'score': 0.6506676,\n",
       " 'raw_content': \"랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) - 테디노트\\n\\nSkip to primary navigation\\nSkip to content\\nSkip to footer\\n\\n테디노트 데이터와 인공지능을 좋아하는 개발자 노트\\n\\n검색\\n카테고리\\n태그\\n연도\\n강의\\n어바웃미\\n\\n토글 메뉴\\n\\nHome \\n/3.  Langchain \\n/5.  랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\\n\\n🔥알림🔥\\n① 테디노트 유튜브 - 구경하러 가기!\\n② LangChain 한국어 튜토리얼 바로가기 👀\\n③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌\\n④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌\\n⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌\\n랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\\n2023년 09월 28일 5 분 소요\\n목차\\n\\n🌱 랭체인의 주요 기능\\n🌱 환경설정\\nAPI KEY 발급\\n모듈 설치(openai, langchain)\\n\\n\\n🔥 ChatOpenAI\\n🔥 프롬프트 템플릿의 활용\\nLLMChain 객체\\n① run()\\n② apply()\\n③ generate()\\n④ 2개 이상의 변수를 템플릿 안에 정의\\n⑤ 스트리밍(streaming)\\n\\n\\n\\n언어 모델을 활용한 애플리케이션 개발을 돕는 프레임워크인 랭체인(LangChain) 에 대해 깊이 있게 다뤄보고자 합니다.\\n튜토리얼은 시리즈 형식으로 구성되어, 시리즈를 거듭하면서 랭체인(LangChain) 을 통해 언어 모델 기반의 애플리케이션 개발은 더욱 간결하고 효과적으로 이루어질 수 있습니다.\\n🌱 랭체인의 주요 기능\\n랭체인을 통해 다음과 같은 특징을 갖는 애플리케이션을 개발할 수 있습니다.\\n\\n문맥 인식: 언어 모델과 다양한 문맥 소스(프롬프트 지시, 예제, 응답의 근거 내용 등)를 연동하며, 사용자의 문맥을 정확히 이해합니다.\\n추론 능력: 제공된 문맥에 기반하여 어떤 대답을 할지, 또는 어떠한 액션을 취할지에 대한 추론이 가능합니다.\\n\\n랭체인의 가치\\n랭체인의 핵심적인 가치는 여러 가지가 있지만, 그 중에서도 두 가지 주요한 점을 꼽자면 다음과 같습니다.\\n\\n구성 요소: 사용자는 언어 모델과의 상호작용을 위해 다양한 구성 요소와 추상화를 활용할 수 있습니다. 이러한 구성 요소는 개별적으로, 또는 랭체인 프레임워크 내에서 모듈식으로 쉽게 활용할 수 있습니다.\\n사용 준비된 체인: 특정 고수준 작업을 수행하기 위해 미리 조립된 구성 요소의 패키지입니다.\\n\\n특히, 이러한 사용 준비된 체인은 초보자도 랭체인을 쉽게 시작할 수 있게 도와주며, 복잡한 애플리케이션을 계획하는 전문가들은 기존 체인을 손쉽게 커스터마이징하거나 새롭게 구축할 수 있게 도와줍니다.\\n🌱 환경설정\\nAPI KEY 발급\\n먼저, openai 의 API KEY 를 발급 받아야 합니다. 발급은 다음의 절차를 통해 진행할 수 있습니다.\\nhttps://platform.openai.com/account/api-keys 로 접속합니다.\\n\\nLog in 버튼을 클릭 후 계정에 로그인 합니다. 계정이 아직 생성되지 않은 경우에는 Sign up 으로 회원가입 후 로그인 합니다.\\n\\n\\n\\n“Create new secret key” 버튼을 클릭하여 새로운 키를 발급합니다.\\n\\n\\n\\nName 에는 발급하는 키에 대한 별칭을 입력합니다.\\n\\n\\n\\n새롭게 발급한 키를 복사합니다. 잃어버리면 다시 발급하여야 하므로, 안전한 곳에 저장해 둡니다.\\n\\n\\n모듈 설치(openai, langchain)\\npip 명령어로 모듈을 설치 합니다. 아나콘다 가상환경에서 설치해도 좋습니다.\\n```\\nopenai 파이썬 패키지 설치\\npip install openai langchain\\n```\\n먼저, 설치한 openai 모듈을 import 한 뒤, 발급받은 API KEY를 다음과 같이 설정합니다.\\n```\\nimport os\\nos.environ['OPENAI_API_KEY'] = 'OPENAI API KEY 입력'\\n```\\n사용 가능한 모델 리스트 출력\\n```\\nimport openai\\nmodel_list = sorted([m['id'] for m in openai.Model.list()['data']])\\nfor m in model_list:\\n    print(m)\\n```\\nada\\nada-code-search-code\\nada-code-search-text\\nada-search-document\\nada-search-query\\nada-similarity\\nbabbage\\nbabbage-002\\nbabbage-code-search-code\\nbabbage-code-search-text\\nbabbage-search-document\\nbabbage-search-query\\nbabbage-similarity\\ncode-davinci-edit-001\\ncode-search-ada-code-001\\ncode-search-ada-text-001\\ncode-search-babbage-code-001\\ncode-search-babbage-text-001\\ncurie\\ncurie-instruct-beta\\ncurie-search-document\\ncurie-search-query\\ncurie-similarity\\ndavinci\\ndavinci-002\\ndavinci-instruct-beta\\ndavinci-search-document\\ndavinci-search-query\\ndavinci-similarity\\ngpt-3.5-turbo\\ngpt-3.5-turbo-0301\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k\\ngpt-3.5-turbo-16k-0613\\ngpt-3.5-turbo-instruct\\ngpt-3.5-turbo-instruct-0914\\ngpt-4\\ngpt-4-0314\\ngpt-4-0613\\ntext-ada-001\\ntext-babbage-001\\ntext-curie-001\\ntext-davinci-001\\ntext-davinci-002\\ntext-davinci-003\\ntext-davinci-edit-001\\ntext-embedding-ada-002\\ntext-search-ada-doc-001\\ntext-search-ada-query-001\\ntext-search-babbage-doc-001\\ntext-search-babbage-query-001\\ntext-search-curie-doc-001\\ntext-search-curie-query-001\\ntext-search-davinci-doc-001\\ntext-search-davinci-query-001\\ntext-similarity-ada-001\\ntext-similarity-babbage-001\\ntext-similarity-curie-001\\ntext-similarity-davinci-001\\nwhisper-1\\n🔥 ChatOpenAI\\nOpenAI 사의 채팅 전용 Large Language Model(llm) 입니다.\\n객체를 생성할 때 다음을 옵션 값을 지정할 수 있습니다. 옵션에 대한 상세 설명은 다음과 같습니다.\\ntemperature\\n\\n사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.\\n\\nmax_tokens\\n\\n채팅 완성에서 생성할 토큰의 최대 개수입니다.\\n\\nmodel_name: 적용 가능한 모델 리스트\\n\\n\\ngpt-3.5-turbo\\n\\n\\ngpt-3.5-turbo-0301\\n\\n\\ngpt-3.5-turbo-0613\\n\\n\\ngpt-3.5-turbo-16k\\n\\n\\ngpt-3.5-turbo-16k-0613\\n\\n\\ngpt-3.5-turbo-instruct\\n\\n\\ngpt-3.5-turbo-instruct-0914\\n\\n\\ngpt-4\\n\\n\\ngpt-4-0314\\n\\n\\ngpt-4-0613\\n\\n\\n```\\nfrom langchain.chat_models import ChatOpenAI\\n객체 생성\\nllm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \\n                 max_tokens=2048,             # 최대 토큰수\\n                 model_name='gpt-3.5-turbo',  # 모델명\\n                )\\n질의내용\\nquestion = '대한민국의 수도는 뭐야?'\\n질의\\nprint(f'[답변]: {llm.predict(question)}')\\n```\\n[답변]: 대한민국의 수도는 서울입니다.\\n🔥 프롬프트 템플릿의 활용\\nPromptTemplate\\n\\n\\n사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\\n\\n\\n사용법\\n\\n\\ntemplate: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 {}는 변수를 나타냅니다.\\n\\n\\ninput_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\\n\\n\\n\\n\\ninput_variables\\n\\n\\ninput_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\\n\\n\\n사용법: 리스트 형식으로 변수 이름을 정의합니다.\\n\\n\\n```\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n질문 템플릿 형식 정의\\ntemplate = '{country}의 수도는 뭐야?'\\n템플릿 완성\\nprompt = PromptTemplate(template=template, input_variables=['country'])\\n```\\nLLMChain 객체\\nLLMChain\\n\\n\\nLLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다\\n\\n\\n사용법\\n\\n\\nprompt: 앞서 정의한 PromptTemplate 객체를 사용합니다.\\n\\n\\nllm: 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다.\\n\\n\\n\\n\\n```\\n연결된 체인(Chain)객체 생성\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n```\\n① run()\\nrun() 함수로 템플릿 프롬프트 실행\\n```\\n체인 실행: run()\\nprint(llm_chain.run(country='일본'))\\n```\\n일본의 수도는 도쿄입니다.\\n```\\n체인 실행: run()\\nprint(llm_chain.run(country='캐나다'))\\n```\\n캐나다의 수도는 오타와(Ottawa)입니다.\\n② apply()\\napply() 함수로 여러개의 입력을 한 번에 실행\\n```\\ninput_list = [\\n    {'country': '호주'},\\n    {'country': '중국'},\\n    {'country': '네덜란드'}\\n]\\nllm_chain.apply(input_list)\\n```\\n[{'text': '호주의 수도는 캔버라입니다.'},\\n {'text': '중국의 수도는 베이징(北京)입니다.'},\\n {'text': '네덜란드의 수도는 암스테르담(Amsterdam)입니다.'}]\\ntext 키 값으로 결과 뭉치가 반환되었음을 확인할 수 있습니다.\\n이를 반복문으로 출력한다면 다음과 같습니다.\\n```\\ninput_list 에 대한 결과 반환\\nresult = llm_chain.apply(input_list)\\n반복문으로 결과 출력\\nfor res in result:\\n    print(res['text'].strip())\\n```\\n호주의 수도는 캔버라입니다.\\n중국의 수도는 베이징(北京)입니다.\\n네덜란드의 수도는 암스테르담(Amsterdam)입니다.\\n③ generate()\\ngenerate() 는 문자열 대신에 LLMResult를 반환하는 점을 제외하고는 apply와 유사합니다.\\nLLMResult는 토큰 사용량과 종료 이유와 같은 유용한 생성 정보를 자주 포함하고 있습니다.\\n```\\ninput_list 에 대한 결과 반환\\ngenerated_result = llm_chain.generate(input_list)\\nprint(generated_result)\\n```\\ngenerations=[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='호주의 수도는 캔버라입니다.', additional_kwargs={}, example=False))], [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.', additional_kwargs={}, example=False))], [ChatGeneration(text='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 58, 'completion_tokens': 57, 'total_tokens': 115}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('957a5369-a20e-470a-bcea-c325b3aafb4a')), RunInfo(run_id=UUID('f5f6f639-76f8-43e3-9103-03aa7eac6fe5')), RunInfo(run_id=UUID('f9c4ce3f-4e5d-47d5-86af-f20c077b754e'))]\\n```\\n답변 출력\\ngenerated_result.generations\\n```\\n[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='호주의 수도는 캔버라입니다.', additional_kwargs={}, example=False))],\\n [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.', additional_kwargs={}, example=False))],\\n [ChatGeneration(text='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', additional_kwargs={}, example=False))]]\\n```\\n토큰 사용량 출력\\ngenerated_result.llm_output\\n```\\n{'token_usage': {'prompt_tokens': 58,\\n  'completion_tokens': 57,\\n  'total_tokens': 115},\\n 'model_name': 'gpt-3.5-turbo'}\\n```\\nrun ID 출력\\ngenerated_result.run\\n```\\n[RunInfo(run_id=UUID('957a5369-a20e-470a-bcea-c325b3aafb4a')),\\n RunInfo(run_id=UUID('f5f6f639-76f8-43e3-9103-03aa7eac6fe5')),\\n RunInfo(run_id=UUID('f9c4ce3f-4e5d-47d5-86af-f20c077b754e'))]\\n```\\n답변 출력\\nfor gen in generated_result.generations:\\n    print(gen[0].text.strip())\\n```\\n호주의 수도는 캔버라입니다.\\n중국의 수도는 베이징(北京)입니다.\\n네덜란드의 수도는 암스테르담(Amsterdam)입니다.\\n④ 2개 이상의 변수를 템플릿 안에 정의\\n2개 이상의 변수를 적용하여 템플릿을 생성할 수 있습니다.\\n이번에는 2개 이상의 변수(input_variables) 를 활용하여 템플릿 구성을 해보겠습니다.\\n```\\n질문 템플릿 형식 정의\\ntemplate = '{area1} 와 {area2} 의 시차는 몇시간이야?'\\n템플릿 완성\\nprompt = PromptTemplate(template=template, input_variables=['area1', 'area2'])\\n연결된 체인(Chain)객체 생성\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n```\\n```\\n체인 실행: run()\\nprint(llm_chain.run(area1='서울', area2='파리'))\\n```\\n서울과 파리의 시차는 8시간입니다. 서울이 파리보다 8시간 앞서 있습니다.\\n```\\ninput_list = [\\n    {'area1': '파리', 'area2': '뉴욕'},\\n    {'area1': '서울', 'area2': '하와이'},\\n    {'area1': '켄버라', 'area2': '베이징'}\\n]\\n반복문으로 결과 출력\\nresult = llm_chain.apply(input_list)\\nfor res in result:\\n    print(res['text'].strip())\\n```\\n파리와 뉴욕의 시차는 일반적으로 6시간입니다. 파리가 뉴욕보다 6시간 앞서 있습니다. 예를 들어, 파리가 오전 9시라면 뉴욕은 오전 3시입니다.\\n서울과 하와이의 시차는 서울이 하와이보다 19시간 빠릅니다. 예를 들어, 서울이 오전 9시라면 하와이는 전날 오후 2시입니다.\\n켄버라와 베이징의 시차는 2시간입니다. 켄버라는 오스트레일리아의 수도로 UTC+10 시간대에 위치하고, 베이징은 중국의 수도로 UTC+8 시간대에 위치합니다.\\n⑤ 스트리밍(streaming)\\n스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용합니다.\\n다음과 같이 streaming=True 로 설정하고 스트리밍으로 답변을 받기 위한 StreamingStdOutCallbackHandler() 을 콜백으로 지정합니다.\\n```\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n객체 생성\\nllm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \\n                 max_tokens=2048,             # 최대 토큰수\\n                 model_name='gpt-3.5-turbo',  # 모델명\\n                 streaming=True,            \\n                 callbacks=[StreamingStdOutCallbackHandler()]\\n                )\\n```\\n```\\n질의내용\\nquestion = '대한민국의 수도는 뭐야?'\\n스트리밍으로 답변 출력\\nresponse = llm.predict(question)\\n```\\n대한민국의 수도는 서울입니다.\\n태그: ChatGPT, ChatOpenAI, GPT3.5, GPT4, langchain, langchain tutorial, OpenAI, 랭체인, 랭체인 튜토리얼\\n카테고리: langchain\\n업데이트: 2023년 09월 28일\\n공유하기\\nTwitter Facebook LinkedIn\\n이전 다음\\n댓글남기기\\n참고\\npoetry 의 거의 모든것 (튜토리얼)\\n2024년 03월 30일 5 분 소요\\nPython 개발에 있어서 poetry는 매우 강력한 도구로, 프로젝트의 의존성 관리와 패키지 배포를 간소화하는 데 큰 도움을 줍니다. 지금부터 poetry 활용 튜토리얼을 살펴 보겠습니다.\\nLangGraph Retrieval Agent를 활용한 동적 문서 검색 및 처리\\n2024년 03월 06일 10 분 소요\\nLangGraph Retrieval Agent는 언어 처리, AI 모델 통합, 데이터베이스 관리, 그래프 기반 데이터 처리 등 다양한 기능을 제공하여 언어 기반 AI 애플리케이션 개발에 필수적인 도구입니다.\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일 35 분 소요\\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일 41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 데이터 처리 작업을 수행하는 방법을 소개합니다. LangSmith 를 사용하여 Agent의 추론 단계를 추적합니다. Agent가 활용할 검색 도구(Tavily Search), PDF 기반 검색 리트리버...\\n\\n팔로우:\\nYouTube\\nGitHub\\nInstagram\\n피드\\n\\n© 2024 테디노트. Powered by Jekyll & Minimal Mistakes.\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 웹 검색 결과의 첫 번째 결과 확인\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac37855",
   "metadata": {},
   "source": [
    "## 그래프 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab91c2",
   "metadata": {},
   "source": [
    "### 그래프 상태 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d23ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "\n",
    "# 그래프의 상태 정의\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    그래프의 상태를 나타내는 데이터 모델\n",
    "\n",
    "    Attributes:\n",
    "        question: 질문\n",
    "        generation: LLM 생성된 답변\n",
    "        documents: 도큐먼트 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[List[str], \"List of documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266cc42",
   "metadata": {},
   "source": [
    "## 그래프 흐름 정의\n",
    "\n",
    "**그래프 흐름**을 정의하여 **Adaptive RAG**의 작동 방식을 명확히 합니다. 이 단계에서는 그래프의 상태와 전환을 설정하여 쿼리 처리의 효율성을 높입니다.\n",
    "\n",
    "- **상태 정의**: 그래프의 각 상태를 명확히 정의하여 쿼리의 진행 상황을 추적합니다.\n",
    "- **전환 설정**: 상태 간의 전환을 설정하여 쿼리가 적절한 경로를 따라 진행되도록 합니다.\n",
    "- **흐름 최적화**: 그래프의 흐름을 최적화하여 정보 검색과 생성의 정확성을 향상시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bf00c",
   "metadata": {},
   "source": [
    "### 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee6f34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# 문서 검색 노드\n",
    "def retrieve(state):\n",
    "    print(\"==== [RETRIEVE] ====\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 문서 검색 수행\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# 답변 생성 노드\n",
    "def generate(state):\n",
    "    print(\"==== [GENERATE] ====\")\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG 답변 생성\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# 문서 관련성 평가 노드\n",
    "def grade_documents(state):\n",
    "    print(\"==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\")\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 각 문서에 대한 관련성 점수 계산\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            # 관련성이 있는 문서 추가\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            # 관련성이 없는 문서는 건너뛰기\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs}\n",
    "\n",
    "\n",
    "# 질문 재작성 노드\n",
    "def transform_query(state):\n",
    "    print(\"==== [TRANSFORM QUERY] ====\")\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 질문 재작성\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n",
    "\n",
    "\n",
    "# 웹 검색 노드\n",
    "def web_search(state):\n",
    "    print(\"==== [WEB SEARCH] ====\")\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 웹 검색 수행\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "    web_results_docs = [\n",
    "        Document(\n",
    "            page_content=web_result[\"content\"],\n",
    "            metadata={\"source\": web_result[\"url\"]},\n",
    "        )\n",
    "        for web_result in web_results\n",
    "    ]\n",
    "\n",
    "    return {\"documents\": web_results_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec62cc",
   "metadata": {},
   "source": [
    "## 추가 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d33976b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 라우팅 노드\n",
    "def route_question(state):\n",
    "    print(\"==== [ROUTE QUESTION] ====\")\n",
    "    # 질문 가져오기\n",
    "    question = state[\"question\"]\n",
    "    # 질문 라우팅\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    # 질문 라우팅 결과에 따른 노드 라우팅\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"==== [ROUTE QUESTION TO WEB SEARCH] ====\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"==== [ROUTE QUESTION TO VECTORSTORE] ====\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "# 문서 관련성 평가 노드\n",
    "def decide_to_generate(state):\n",
    "    print(\"==== [DECISION TO GENERATE] ====\")\n",
    "    # 문서 검색 결과 가져오기\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # 모든 문서가 관련성 없는 경우 질문 재작성\n",
    "        print(\n",
    "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # 관련성 있는 문서가 있는 경우 답변 생성\n",
    "        print(\"==== [DECISION: GENERATE] ====\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def hallucination_check(state):\n",
    "    print(\"==== [CHECK HALLUCINATIONS] ====\")\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # 환각 평가\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Hallucination 여부 확인\n",
    "    if grade == \"yes\":\n",
    "        print(\"==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\")\n",
    "\n",
    "        # 답변의 관련성(Relevance) 평가\n",
    "        print(\"==== [GRADE GENERATED ANSWER vs QUESTION] ====\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "\n",
    "        # 관련성 평가 결과에 따른 처리\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====\")\n",
    "            return \"relevant\"\n",
    "        else:\n",
    "            print(\"==== [DECISION: GENERATED ANSWER DOES NOT ADDRESS QUESTION] ====\")\n",
    "            return \"not relevant\"\n",
    "    else:\n",
    "        print(\"==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====\")\n",
    "        return \"hallucination\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412119d",
   "metadata": {},
   "source": [
    "### 그래프 컴파일\n",
    "\n",
    "**그래프 컴파일** 단계에서는 **Adaptive RAG**의 워크플로우를 구축하고 실행 가능한 상태로 만듭니다. 이 과정은 그래프의 각 노드와 엣지를 연결하여 쿼리 처리의 전체 흐름을 정의합니다.\n",
    "\n",
    "- **노드 정의**: 각 노드를 정의하여 그래프의 상태와 전환을 명확히 합니다.\n",
    "- **엣지 설정**: 노드 간의 엣지를 설정하여 쿼리가 적절한 경로를 따라 진행되도록 합니다.\n",
    "- **워크플로우 구축**: 그래프의 전체 흐름을 구축하여 정보 검색과 생성의 효율성을 극대화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c106a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 그래프 상태 초기화\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"web_search\", web_search)  # 웹 검색\n",
    "workflow.add_node(\"retrieve\", retrieve)  # 문서 검색\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # 문서 평가\n",
    "workflow.add_node(\"generate\", generate)  # 답변 생성\n",
    "workflow.add_node(\"transform_query\", transform_query)  # 쿼리 변환\n",
    "\n",
    "# 그래프 빌드\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",  # 웹 검색으로 라우팅\n",
    "        \"vectorstore\": \"retrieve\",  # 벡터스토어로 라우팅\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")  # 웹 검색 후 답변 생성\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")  # 문서 검색 후 평가\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",  # 쿼리 변환 필요\n",
    "        \"generate\": \"generate\",  # 답변 생성 가능\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")  # 쿼리 변환 후 문서 검색\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    hallucination_check,\n",
    "    {\n",
    "        \"hallucination\": \"generate\",  # Hallucination 발생 시 재생성\n",
    "        \"relevant\": END,  # 답변의 관련성 여부 통과\n",
    "        \"not relevant\": \"transform_query\",  # 답변의 관련성 여부 통과 실패 시 쿼리 변환\n",
    "    },\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f4505",
   "metadata": {},
   "source": [
    "그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46ce79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Visualize Graph Error: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2739b",
   "metadata": {},
   "source": [
    "## 그래프 사용\n",
    "\n",
    "**그래프 사용** 단계에서는 **Adaptive RAG**의 실행을 통해 쿼리 처리 결과를 확인합니다. 이 과정은 그래프의 각 노드와 엣지를 따라 쿼리를 처리하여 최종 결과를 생성합니다.\n",
    "\n",
    "- **그래프 실행**: 정의된 그래프를 실행하여 쿼리의 흐름을 따라갑니다.\n",
    "- **결과 확인**: 그래프 실행 후 생성된 결과를 검토하여 쿼리가 적절히 처리되었는지 확인합니다.\n",
    "- **결과 분석**: 생성된 결과를 분석하여 쿼리의 목적에 부합하는지 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b020b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== [ROUTE QUESTION] ====\n",
      "==== [ROUTE QUESTION TO VECTORSTORE] ====\n",
      "==== [RETRIEVE] ====\n",
      "==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "==== [DECISION TO GENERATE] ====\n",
      "==== [DECISION: GENERATE] ====\n",
      "==== [GENERATE] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "삼성전자가 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023년12월호_F.pdf (page 12)==== [CHECK HALLUCINATIONS] ====\n",
      "==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\n",
      "==== [GRADE GENERATED ANSWER vs QUESTION] ====\n",
      "==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"삼성전자가 개발한 생성형 AI 의 이름은?\",\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(app, inputs, config, [\"agent\", \"rewrite\", \"generate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e25d23b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== [ROUTE QUESTION] ====\n",
      "==== [ROUTE QUESTION TO WEB SEARCH] ====\n",
      "==== [WEB SEARCH] ====\n",
      "==== [GENERATE] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "2024년 노벨 문학상 수상자는 한국 작가 한강입니다. \n",
      "\n",
      "**Source**\n",
      "- https://www.yna.co.kr/view/MYH20241010024300704==== [CHECK HALLUCINATIONS] ====\n",
      "==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\n",
      "==== [GRADE GENERATED ANSWER vs QUESTION] ====\n",
      "==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====\n"
     ]
    }
   ],
   "source": [
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"2024년 노벨 문학상 수상자는 누구인가요?\",\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(app, inputs, config, [\"agent\", \"rewrite\", \"generate\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu-rag-j_hBGe1v-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
